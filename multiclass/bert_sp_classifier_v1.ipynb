{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04169412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "MODEL_NAME = \"Rostlab/prot_bert\" \n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\" # using mps instead of cuda for training on mac\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "NUM_CLASSES = 6  # num classes for classification\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 7\n",
    "LR = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cc725e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 25693\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "uniprot_ac",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "kingdom",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sequence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a0c34580-4c39-4de0-8de5-3fa812dd6a43",
       "rows": [
        [
         "0",
         "Q8TF40",
         "EUKARYA",
         "NO_SP",
         "MAPTLFQKLFSKRTGLGAPGRDARDPDCGFSWPLPEFDPSQIRLIVYQDCERRGRNVLFDSSVKRRNEDI",
         "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII"
        ],
        [
         "1",
         "Q1ENB6",
         "EUKARYA",
         "NO_SP",
         "MDFTSLETTTFEEVVIALGSNVGNRMNNFKEALRLMKDYGISVTRHSCLYETEPVHVTDQPRFLNAAIRG",
         "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII"
        ],
        [
         "2",
         "P36001",
         "EUKARYA",
         "NO_SP",
         "MDDISGRQTLPRINRLLEHVGNPQDSLSILHIAGTNGKETVSKFLTSILQHPGQQRQRVLIGRYTTSSLL",
         "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII"
        ],
        [
         "3",
         "P55317",
         "EUKARYA",
         "NO_SP",
         "MLGTVKMEGHETSDWNSYYADTQEAYSSVPVSNMNSGLGSMNSMNTYMTMNTMTTSGNMTPASFNMSYAN",
         "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII"
        ],
        [
         "4",
         "P35583",
         "EUKARYA",
         "NO_SP",
         "MLGAVKMEGHEPSDWSSYYAEPEGYSSVSNMNAGLGMNGMNTYMSMSAAAMGGGSGNMSAGSMNMSSYVG",
         "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_ac</th>\n",
       "      <th>kingdom</th>\n",
       "      <th>type</th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q8TF40</td>\n",
       "      <td>EUKARYA</td>\n",
       "      <td>NO_SP</td>\n",
       "      <td>MAPTLFQKLFSKRTGLGAPGRDARDPDCGFSWPLPEFDPSQIRLIV...</td>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1ENB6</td>\n",
       "      <td>EUKARYA</td>\n",
       "      <td>NO_SP</td>\n",
       "      <td>MDFTSLETTTFEEVVIALGSNVGNRMNNFKEALRLMKDYGISVTRH...</td>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P36001</td>\n",
       "      <td>EUKARYA</td>\n",
       "      <td>NO_SP</td>\n",
       "      <td>MDDISGRQTLPRINRLLEHVGNPQDSLSILHIAGTNGKETVSKFLT...</td>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P55317</td>\n",
       "      <td>EUKARYA</td>\n",
       "      <td>NO_SP</td>\n",
       "      <td>MLGTVKMEGHETSDWNSYYADTQEAYSSVPVSNMNSGLGSMNSMNT...</td>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P35583</td>\n",
       "      <td>EUKARYA</td>\n",
       "      <td>NO_SP</td>\n",
       "      <td>MLGAVKMEGHEPSDWSSYYAEPEGYSSVSNMNAGLGMNGMNTYMSM...</td>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  uniprot_ac  kingdom   type  \\\n",
       "0     Q8TF40  EUKARYA  NO_SP   \n",
       "1     Q1ENB6  EUKARYA  NO_SP   \n",
       "2     P36001  EUKARYA  NO_SP   \n",
       "3     P55317  EUKARYA  NO_SP   \n",
       "4     P35583  EUKARYA  NO_SP   \n",
       "\n",
       "                                            sequence  \\\n",
       "0  MAPTLFQKLFSKRTGLGAPGRDARDPDCGFSWPLPEFDPSQIRLIV...   \n",
       "1  MDFTSLETTTFEEVVIALGSNVGNRMNNFKEALRLMKDYGISVTRH...   \n",
       "2  MDDISGRQTLPRINRLLEHVGNPQDSLSILHIAGTNGKETVSKFLT...   \n",
       "3  MLGTVKMEGHETSDWNSYYADTQEAYSSVPVSNMNSGLGSMNSMNT...   \n",
       "4  MLGAVKMEGHEPSDWSSYYAEPEGYSSVSNMNAGLGMNGMNTYMSM...   \n",
       "\n",
       "                                               label  \n",
       "0  IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...  \n",
       "1  IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...  \n",
       "2  IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...  \n",
       "3  IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...  \n",
       "4  IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "records = []  # uniprot_ac, kingdom, type_, sequence, label\n",
    "with open(\"/Users/jonas/Desktop/Uni/PBL/data/complete_set_unpartitioned.fasta\", \"r\") as f:\n",
    "    current_record = None\n",
    "    for line in f:\n",
    "        if line.startswith(\">\"):\n",
    "            if current_record is not None:\n",
    "                if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
    "                    # Save the previous record before starting a new one\n",
    "                    records.append(current_record)\n",
    "                else:\n",
    "                    # If the previous record is incomplete, skip it\n",
    "                    print(\"Skipping incomplete record:\", current_record)\n",
    "            # Start a new record\n",
    "            uniprot_ac, kingdom, type_ = line[1:].strip().split(\"|\")\n",
    "            current_record = {\"uniprot_ac\": uniprot_ac, \"kingdom\": kingdom, \"type\": type_, \"sequence\": None, \"label\": None}\n",
    "        else:\n",
    "            # Check if the line contains a sequence or a label\n",
    "            if current_record[\"sequence\"] is None:\n",
    "                current_record[\"sequence\"] = line.strip()\n",
    "            elif current_record[\"label\"] is None:\n",
    "                current_record[\"label\"] = line.strip()\n",
    "            else:\n",
    "                # If both sequence and label are already set, skip this line\n",
    "                print(\"Skipping extra line in record:\", current_record)\n",
    "    # Save the last record if it's complete\n",
    "    if current_record is not None:\n",
    "        if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
    "            records.append(current_record)\n",
    "        else:\n",
    "            print(\"Skipping incomplete record:\", current_record)\n",
    "\n",
    "\"\"\"\n",
    "# Save the DataFrame to a CSV file\n",
    "df_raw.to_csv(\"/Users/jonas/Desktop/Uni/PBL/data/complete_set_unpartitioned.csv\", index=False)\n",
    "\"\"\"\n",
    "# Print the number of records\n",
    "print(f\"Total records: {len(records)}\")\n",
    "df_raw = pd.DataFrame(records)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f89f4952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "uniprot_ac",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "kingdom",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "sequence",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "6b2e2aa9-cbb7-4c35-9a79-015542409a72",
       "rows": [
        [
         "count",
         "25580",
         "25580",
         "25580",
         "25580",
         "25580"
        ],
        [
         "unique",
         "25580",
         "4",
         "5",
         "24367",
         "1878"
        ],
        [
         "top",
         "Q8TF40",
         "EUKARYA",
         "NO_SP",
         "MKLSRRSFMKANAVAAAAAAAGLSVPGVARAVVGQQEAIKWDKAPCRFCGTGCGVLVGTQQGRVVACQGD",
         "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII"
        ],
        [
         "freq",
         "1",
         "20423",
         "19036",
         "41",
         "16382"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_ac</th>\n",
       "      <th>kingdom</th>\n",
       "      <th>type</th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25580</td>\n",
       "      <td>25580</td>\n",
       "      <td>25580</td>\n",
       "      <td>25580</td>\n",
       "      <td>25580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>25580</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>24367</td>\n",
       "      <td>1878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Q8TF40</td>\n",
       "      <td>EUKARYA</td>\n",
       "      <td>NO_SP</td>\n",
       "      <td>MKLSRRSFMKANAVAAAAAAAGLSVPGVARAVVGQQEAIKWDKAPC...</td>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>20423</td>\n",
       "      <td>19036</td>\n",
       "      <td>41</td>\n",
       "      <td>16382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uniprot_ac  kingdom   type  \\\n",
       "count       25580    25580  25580   \n",
       "unique      25580        4      5   \n",
       "top        Q8TF40  EUKARYA  NO_SP   \n",
       "freq            1    20423  19036   \n",
       "\n",
       "                                                 sequence  \\\n",
       "count                                               25580   \n",
       "unique                                              24367   \n",
       "top     MKLSRRSFMKANAVAAAAAAAGLSVPGVARAVVGQQEAIKWDKAPC...   \n",
       "freq                                                   41   \n",
       "\n",
       "                                                    label  \n",
       "count                                               25580  \n",
       "unique                                               1878  \n",
       "top     IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII...  \n",
       "freq                                                16382  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_raw[~df_raw[\"label\"].str.contains(\"P\")]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "568deead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "uniprot_ac",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "kingdom",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "sequence",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "4a022c54-8337-4f69-83b9-04e950de012b",
       "rows": [
        [
         "count",
         "25580",
         "25580",
         "25580",
         "25580",
         "25580"
        ],
        [
         "unique",
         "25580",
         "4",
         "5",
         "24367",
         "1878"
        ],
        [
         "top",
         "Q8TF40",
         "EUKARYA",
         "NO_SP",
         "MKLSRRSFMKANAVAAAAAAAGLSVPGVARAVVGQQEAIKWDKAPCRFCGTGCGVLVGTQQGRVVACQGD",
         "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]"
        ],
        [
         "freq",
         "1",
         "20423",
         "19036",
         "41",
         "16382"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_ac</th>\n",
       "      <th>kingdom</th>\n",
       "      <th>type</th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25580</td>\n",
       "      <td>25580</td>\n",
       "      <td>25580</td>\n",
       "      <td>25580</td>\n",
       "      <td>25580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>25580</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>24367</td>\n",
       "      <td>1878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Q8TF40</td>\n",
       "      <td>EUKARYA</td>\n",
       "      <td>NO_SP</td>\n",
       "      <td>MKLSRRSFMKANAVAAAAAAAGLSVPGVARAVVGQQEAIKWDKAPC...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>20423</td>\n",
       "      <td>19036</td>\n",
       "      <td>41</td>\n",
       "      <td>16382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uniprot_ac  kingdom   type  \\\n",
       "count       25580    25580  25580   \n",
       "unique      25580        4      5   \n",
       "top        Q8TF40  EUKARYA  NO_SP   \n",
       "freq            1    20423  19036   \n",
       "\n",
       "                                                 sequence  \\\n",
       "count                                               25580   \n",
       "unique                                              24367   \n",
       "top     MKLSRRSFMKANAVAAAAAAAGLSVPGVARAVVGQQEAIKWDKAPC...   \n",
       "freq                                                   41   \n",
       "\n",
       "                                                    label  \n",
       "count                                               25580  \n",
       "unique                                               1878  \n",
       "top     [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...  \n",
       "freq                                                16382  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {'S': 0, 'T': 1, 'L': 2, 'I': 3, 'M': 4, 'O': 5}\n",
    "\n",
    "df_encoded = df.copy()\n",
    "df_encoded[\"label\"] = df_encoded[\"label\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
    "df_encoded = df_encoded[df_encoded[\"label\"].map(len) > 0]  # Remove rows with empty label lists\n",
    "\n",
    "# make random smaller dataset\n",
    "#df_encoded = df_encoded.sample(frac=0.4, random_state=42)\n",
    "\n",
    "sequences = df_encoded[\"sequence\"].tolist()\n",
    "label_seqs = df_encoded[\"label\"].tolist()\n",
    "\n",
    "df_encoded.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5bfa5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30, 1024, padding_idx=0)\n",
       "    (position_embeddings): Embedding(40000, 1024)\n",
       "    (token_type_embeddings): Embedding(2, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-29): 30 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False )\n",
    "encoder = BertModel.from_pretrained(MODEL_NAME)\n",
    "encoder.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0dcdebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify by sequence length to avoid ValueError\n",
    "train_seqs, test_seqs, train_label_seqs, test_label_seqs = train_test_split(\n",
    "    sequences, label_seqs, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44ab5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPDataset(Dataset):\n",
    "    def __init__(self, sequences, label_seqs, label_map):\n",
    "        self.label_map = label_map\n",
    "        self.label_seqs = label_seqs\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        # preprocess the sequence (insert spaces between amino acids)\n",
    "        seq_processed = \" \".join(list(seq))\n",
    "        labels = self.label_seqs[idx]\n",
    "        # Tokenize the sequence\n",
    "        encoded = tokenizer(seq_processed, return_tensors=\"pt\",\n",
    "                            padding=\"max_length\", truncation=True, max_length=512)\n",
    "        input_ids = encoded['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Build a label tensor of the same length as input_ids.\n",
    "        # For tokens beyond the original sequence length, assign -100 so that loss ignores these.\n",
    "        orig_length = len(seq)\n",
    "        token_labels = []\n",
    "        \n",
    "        for i in range(input_ids.size(0)):\n",
    "            if i == 0 or i > orig_length:  \n",
    "                token_labels.append(-100)  # ignore [CLS] or padding tokens\n",
    "            else:\n",
    "                # Use the already encoded label directly\n",
    "                token_labels.append(labels[i-1])\n",
    "        labels_tensor = torch.tensor(token_labels)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels_tensor\n",
    "        }\n",
    "\n",
    "train_dataset = SPDataset(train_seqs, train_label_seqs, label_map)\n",
    "test_dataset = SPDataset(test_seqs, test_label_seqs, label_map)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd916c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the model on top of the prot_bert encoder\n",
    "# using the encoder as a protein feature extractor\n",
    "# and adding a (linear) classifier on top\n",
    "\n",
    "class SPClassifier(nn.Module):\n",
    "    def __init__(self, encoder_model):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder_model # this is just the bert model\n",
    "        self.dropout = nn.Dropout(0.3) # zero out 30% of the neuron outputs to prevent overfitting\n",
    "        # Ensure the classifier input dimension matches the ProtBERT hidden size\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(), # negative values are set to 0\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    # takes encoder output and applies the classifier\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        encoder_output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = encoder_output.last_hidden_state  # shape: (batch, seq_len, hidden)\n",
    "        output = self.classifier(self.dropout(hidden_states))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Initialize the model\n",
    "model = SPClassifier(encoder).to(DEVICE)\n",
    "\n",
    "# optimizer and Loss\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LR,           # Learning rate\n",
    "    betas=(0.85, 0.999),  # momentum\n",
    "    eps=1e-6,          # epsilon\n",
    "    weight_decay=0.01  # regularization\n",
    ")\n",
    "\n",
    "# scheduler for learning rate\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(train_loader) * EPOCHS\n",
    ")\n",
    "\n",
    "# Counter({'I': 1204001, 'O': 362643, 'S': 85526, 'M': 74445, 'L': 46065, 'T': 22272, 'P': 951})\n",
    "class_counts = [1204001, 85526, 22272, 46865, 74445, 362643]  # Count for each class (I, S, T, L, M, O)\n",
    "\n",
    "# hopefully deals with the class imbalance -> loss func acknowledges the imbalance with class weights \n",
    "# most frquent class gets the lowest weight here\n",
    "weights = torch.tensor([1.0 / count for count in class_counts], device=DEVICE)\n",
    "\n",
    "# loss function that ignores the padding tokens (-100)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights, ignore_index=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5623585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3051\n",
      "Epoch 2, Loss: 0.5268\n",
      "Epoch 3, Loss: 0.1495\n",
      "Epoch 4, Loss: 0.0853\n",
      "Epoch 5, Loss: 0.0756\n",
      "Epoch 6, Loss: 0.0706\n",
      "Epoch 7, Loss: 0.0679\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"/Users/jonas/Desktop/Uni/PBL/logs/prot_bert_linear_classifier_v1\")\n",
    "\n",
    "# Training Loop\n",
    "model.train()\n",
    "\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # unfreeze the last layer of the encoder after 2 epochs abd the rest after 7 epochs\n",
    "    if epoch == 2:\n",
    "        for param in model.encoder.encoder.layer[-1:].parameters():\n",
    "            param.requires_grad = True\n",
    "    elif epoch == 7:\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    total_loss = 0 # total epoch loss\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        token_labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()  # resets gradients\n",
    "        logits = model(input_ids, attention_mask)  # forward pass\n",
    "        loss = loss_fn(logits.view(-1, NUM_CLASSES), token_labels.view(-1)) # flatten logits and labels and compute loss\n",
    "        loss.backward()  # backpropagation\n",
    "        optimizer.step()  # update weights\n",
    "        scheduler.step() # update learning rate\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_train_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6707d9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.51      0.67     25619\n",
      "           1       0.74      0.94      0.83      6739\n",
      "           2       0.66      0.98      0.79     14328\n",
      "           3       0.94      0.99      0.97    358968\n",
      "           4       0.80      0.86      0.83     21858\n",
      "           5       0.97      0.81      0.88    108891\n",
      "\n",
      "    accuracy                           0.93    536403\n",
      "   macro avg       0.85      0.85      0.83    536403\n",
      "weighted avg       0.93      0.93      0.92    536403\n",
      "\n",
      "Epoch 7, Val Loss: 0.0662, Val Acc: 0.9264\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "all_preds = [] # predicted labels\n",
    "all_labels = [] # true types\n",
    "with torch.no_grad():\n",
    "    # uses the batches from the test set for eval\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids, attention_mask) # runs forward pass\n",
    "        preds = torch.argmax(logits, dim=-1) # get per token predicted labels\n",
    "\n",
    "        loss = loss_fn(logits.view(-1, NUM_CLASSES), labels.view(-1)) # calculate loss\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # flatten the predictions and labels\n",
    "        preds_flat = preds.view(-1)\n",
    "        labels_flat = labels.view(-1)\n",
    "        valid_idx = labels_flat != -100 # exclude padding tokens (-100)\n",
    "        all_preds.extend(preds_flat[valid_idx].cpu().numpy())\n",
    "        all_labels.extend(labels_flat[valid_idx].cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "avg_val_loss = val_loss / len(test_loader)\n",
    "val_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Epoch {epoch+1}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "writer.add_scalar(\"Loss/val\", avg_val_loss, epoch)\n",
    "writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "909ee4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), \"/Users/jonas/Desktop/Uni/PBL/models/prot_bert_linear_classifier_v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e612fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Level Accuracy: 0.7131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# compute percentage of false predicted labels\n",
    "def sequence_level_accuracy(predictions, labels):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for pred, label in zip(predictions, labels):\n",
    "        if len(pred) != len(label):\n",
    "            min_len = min(len(pred), len(label))\n",
    "            pred = pred[:min_len]\n",
    "            label = label[:min_len]\n",
    "        total += 1\n",
    "        if (pred == label).all():\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "acc = sequence_level_accuracy(all_preds, all_labels)\n",
    "print(f\"Sequence Level Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
