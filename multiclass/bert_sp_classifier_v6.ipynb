{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04169412",
      "metadata": {
        "id": "04169412"
      },
      "outputs": [],
      "source": [
        "import tqdm as notebook_tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "MODEL_NAME = \"Rostlab/ProtT5\"\n",
        "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\" # using mps instead of cuda for training on mac\n",
        "#DEVICE = \"cpu\"  # use GPU if available, otherwise CPU\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "NUM_CLASSES = 6  # num classes for classification\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "LR = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc725e0",
      "metadata": {
        "id": "4cc725e0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "records = []  # uniprot_ac, kingdom, type_, sequence, label\n",
        "with open(\"/Users/jonas/Desktop/Uni/PBL/data/complete_set_unpartitioned.fasta\", \"r\") as f:\n",
        "    current_record = None\n",
        "    for line in f:\n",
        "        if line.startswith(\">\"):\n",
        "            if current_record is not None:\n",
        "                if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "                    # Save the previous record before starting a new one\n",
        "                    records.append(current_record)\n",
        "                else:\n",
        "                    # If the previous record is incomplete, skip it\n",
        "                    print(\"Skipping incomplete record:\", current_record)\n",
        "            # Start a new record\n",
        "            uniprot_ac, kingdom, type_ = line[1:].strip().split(\"|\")\n",
        "            current_record = {\"uniprot_ac\": uniprot_ac, \"kingdom\": kingdom, \"type\": type_, \"sequence\": None, \"label\": None}\n",
        "        else:\n",
        "            # Check if the line contains a sequence or a label\n",
        "            if current_record[\"sequence\"] is None:\n",
        "                current_record[\"sequence\"] = line.strip()\n",
        "            elif current_record[\"label\"] is None:\n",
        "                current_record[\"label\"] = line.strip()\n",
        "            else:\n",
        "                # If both sequence and label are already set, skip this line\n",
        "                print(\"Skipping extra line in record:\", current_record)\n",
        "    # Save the last record if it's complete\n",
        "    if current_record is not None:\n",
        "        if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "            records.append(current_record)\n",
        "        else:\n",
        "            print(\"Skipping incomplete record:\", current_record)\n",
        "\n",
        "\"\"\"\n",
        "# Save the DataFrame to a CSV file\n",
        "df_raw.to_csv(\"/Users/jonas/Desktop/Uni/PBL/data/complete_set_unpartitioned.csv\", index=False)\n",
        "\"\"\"\n",
        "# Print the number of records\n",
        "print(f\"Total records: {len(records)}\")\n",
        "df_raw = pd.DataFrame(records)\n",
        "df_raw.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89f4952",
      "metadata": {
        "id": "f89f4952"
      },
      "outputs": [],
      "source": [
        "df = df_raw[~df_raw[\"label\"].str.contains(\"P\")]\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e90c49e2",
      "metadata": {
        "id": "e90c49e2"
      },
      "outputs": [],
      "source": [
        "# perform oversampling\n",
        "from sklearn.utils import resample\n",
        "# Separate majority and minority classes\n",
        "df_majority = df[df[\"type\"] == \"NO_SP\"]\n",
        "df_minority = df[df[\"type\"] != \"NO_SP\"]\n",
        "\n",
        "# Upsample minority class\n",
        "df_minority_upsampled = resample(df_minority,\n",
        "                                  replace=True,     # sample with replacement\n",
        "                                  n_samples=len(df_majority),    # to match majority class\n",
        "                                  random_state=42) # reproducible results\n",
        "# Combine majority class with upsampled minority class\n",
        "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        "# Shuffle the dataset\n",
        "df_upsampled = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# total records after oversampling\n",
        "print(f\"Total records after oversampling: {len(df_upsampled)}\")\n",
        "\n",
        "# majority class distribution\n",
        "print(\"Class distribution after oversampling:\")\n",
        "print(df_upsampled[\"type\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "568deead",
      "metadata": {
        "id": "568deead"
      },
      "outputs": [],
      "source": [
        "label_map = {'S': 0, 'T': 1, 'L': 2, 'I': 3, 'M': 4, 'O': 5}\n",
        "\n",
        "df_encoded = df.copy()\n",
        "df_encoded[\"label\"] = df_encoded[\"label\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
        "df_encoded = df_encoded[df_encoded[\"label\"].map(len) > 0]  # Remove rows with empty label lists\n",
        "\n",
        "# make random smaller dataset\n",
        "#df_encoded = df_encoded.sample(frac=0.4, random_state=42)\n",
        "\n",
        "sequences = df_encoded[\"sequence\"].tolist()\n",
        "label_seqs = df_encoded[\"label\"].tolist()\n",
        "\n",
        "df_encoded.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5bfa5c3",
      "metadata": {
        "id": "e5bfa5c3"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
        "encoder = BertModel.from_pretrained(MODEL_NAME)\n",
        "encoder.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dcdebd8",
      "metadata": {
        "id": "0dcdebd8"
      },
      "outputs": [],
      "source": [
        "# Stratify by sequence length to avoid ValueError\n",
        "train_seqs, test_seqs, train_label_seqs, test_label_seqs = train_test_split(\n",
        "    sequences, label_seqs, test_size=0.3, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44ab5146",
      "metadata": {
        "id": "44ab5146"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "class SPDataset(Dataset):\n",
        "    def __init__(self, sequences, label_seqs, label_map):\n",
        "        self.label_map = label_map\n",
        "        self.label_seqs = label_seqs\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        # preprocess the sequence (insert spaces between amino acids)\n",
        "        seq_processed = \" \".join(list(seq))\n",
        "        labels = self.label_seqs[idx]\n",
        "        # Tokenize the sequence (padding to ensure all sequences are the same length -> 512 tokens)\n",
        "        encoded = tokenizer(seq_processed, return_tensors=\"pt\",\n",
        "                            padding=\"max_length\", truncation=True, max_length=512)\n",
        "        input_ids = encoded['input_ids'].squeeze(0)\n",
        "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
        "\n",
        "        # Build a label tensor of the same length as input_ids.\n",
        "        # For tokens beyond the original sequence length, assign -100 so that loss func ignores them.\n",
        "        orig_length = len(seq)\n",
        "        token_labels = []\n",
        "\n",
        "        for i in range(input_ids.size(0)):\n",
        "            if i == 0 or i > orig_length:\n",
        "                token_labels.append(-100)  # ignore padding tokens\n",
        "            else:\n",
        "                # Use the already encoded label directly\n",
        "                token_labels.append(labels[i-1])\n",
        "        labels_tensor = torch.tensor(token_labels)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids, # tokenized and padded\n",
        "            'attention_mask': attention_mask, # differentiate between padding and non-padding tokens\n",
        "            'labels': labels_tensor # aligned label tensor\n",
        "        }\n",
        "\n",
        "train_dataset = SPDataset(train_seqs, train_label_seqs, label_map)\n",
        "test_dataset = SPDataset(test_seqs, test_label_seqs, label_map)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd916c0f",
      "metadata": {
        "id": "bd916c0f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchcrf import CRF\n",
        "\n",
        "class SPCNNClassifier(nn.Module):\n",
        "    def __init__(self, encoder_model, num_labels):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder_model\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        hidden_size = self.encoder.config.hidden_size\n",
        "        # detects local features in the sequence\n",
        "        self.conv = nn.Conv1d(in_channels=hidden_size, out_channels=1024, kernel_size=8, dilation=2, padding=7)\n",
        "        # Normalize the convolution output (expects shape: (batch, 1024, seq_len))\n",
        "        self.bn_conv = nn.BatchNorm1d(1024)\n",
        "        # 2 layer long short term memory network\n",
        "        self.lstm = nn.LSTM(input_size=1024, hidden_size=512, num_layers=3, bidirectional=True, batch_first=True)\n",
        "        # dense layer\n",
        "        self.classifier = nn.Linear(512 * 2, num_labels)\n",
        "        self.crf = CRF(num_labels, batch_first=True)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Encode with BERT\n",
        "        encoder_output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = encoder_output.last_hidden_state  # (batch, seq_len, hidden_size)\n",
        "\n",
        "        #  CNN 1\n",
        "        # Apply conv, then batch normalization and ReLU\n",
        "        x_conv = self.conv(hidden_states.transpose(1, 2))  # (batch, 1024, seq_len)\n",
        "        x_conv = self.bn_conv(x_conv)\n",
        "        x_conv = F.relu(x_conv)                          # (batch, 1024, seq_len)\n",
        "\n",
        "        # Transpose CNN output for LSTM: (batch, seq_len, features)\n",
        "        x_lstm_input = x_conv.transpose(1, 2)           # (batch, seq_len, 1024)\n",
        "\n",
        "        # Apply BiLSTM\n",
        "        lstm_out, _ = self.lstm(x_lstm_input)            # (batch, seq_len, 1024)\n",
        "\n",
        "        # Classifier to num_labels\n",
        "        x_linear = self.classifier(lstm_out)             # (batch, seq_len, num_labels)\n",
        "        logits = self.dropout(x_linear)                  # (batch, seq_len, num_labels)\n",
        "\n",
        "        if labels is not None:\n",
        "            # Replace ignore-index (-100) with a valid label (0) since CRF doesn't support -100\n",
        "            mod_labels = labels.clone()\n",
        "            mod_labels[labels == -100] = 0\n",
        "            loss = -self.crf(logits, mod_labels, mask=attention_mask.bool(), reduction='mean')\n",
        "            return loss\n",
        "        else:\n",
        "            predictions = self.crf.decode(logits, mask=attention_mask.bool())\n",
        "            return predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a7bd31",
      "metadata": {
        "id": "76a7bd31"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Initialize the model\n",
        "model = SPCNNClassifier(encoder, NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {\"params\": model.encoder.encoder.layer[-4:].parameters(), \"lr\": 5e-6},\n",
        "    {\"params\": model.conv.parameters(), \"lr\": 1e-3},\n",
        "    {\"params\": model.classifier.parameters(), \"lr\": 1e-3},\n",
        "    {\"params\": model.lstm.parameters(), \"lr\": 1e-3},\n",
        "    {\"params\": model.crf.parameters(), \"lr\": 1e-3},\n",
        "], weight_decay=0.01)  # adjust weight_decay as needed\n",
        "\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "# scheduler for learning rate\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a9e146b",
      "metadata": {
        "id": "0a9e146b"
      },
      "outputs": [],
      "source": [
        "# Compute sequence-level accuracy, skipping -100 (ignored) positions\n",
        "def sequence_level_accuracy(preds_flat, labels_flat, test_label_seqs):\n",
        "    # reconstruct the sequences from the flat predictions\n",
        "    seq_lengths = [len(seq) for seq in test_label_seqs]\n",
        "    preds_seq = []\n",
        "    labels_seq = []\n",
        "    idx = 0\n",
        "    for l in seq_lengths:\n",
        "        preds_seq.append(preds_flat[idx:idx+l])\n",
        "        labels_seq.append(labels_flat[idx:idx+l])\n",
        "        idx += l\n",
        "\n",
        "    # check if the valid predictions match the labels\n",
        "    correct = 0\n",
        "    for pred, label in zip(preds_seq, labels_seq):\n",
        "        is_valid = [l != -100 for l in label]\n",
        "        valid_preds = [p for p, valid in zip(pred, is_valid) if valid]\n",
        "        valid_labels = [l for l, valid in zip(label, is_valid) if valid]\n",
        "        if valid_preds == valid_labels:\n",
        "            correct += 1\n",
        "\n",
        "    total = len(seq_lengths)\n",
        "    return correct / total if total > 0 else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a9e3006",
      "metadata": {
        "id": "7a9e3006"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "def evaluate_model(model, data_loader, loss_fn, test_label_seqs):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for batch in tqdm(data_loader, desc=\"Evaluation\", leave=False):\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = loss_fn(logits.reshape(-1, NUM_CLASSES), labels.reshape(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        # Flatten and filter out padding tokens (-100)\n",
        "        preds_flat = preds.view(-1)\n",
        "        labels_flat = labels.view(-1)\n",
        "        valid_idx = labels_flat != -100\n",
        "        all_preds.extend(preds_flat[valid_idx].cpu().numpy())\n",
        "        all_labels.extend(labels_flat[valid_idx].cpu().numpy())\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    report = classification_report(all_labels, all_preds, target_names=list(label_map.keys()))\n",
        "\n",
        "    try:\n",
        "        mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "    except ImportError:\n",
        "        mcc = None\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Token-level Accuracy: {acc:.4f}\")\n",
        "    print(f\"Matthews Correlation Coefficient (MCC): {mcc if mcc is not None else 'N/A'}\")\n",
        "\n",
        "    # Compute sequence-level accuracy if the function is defined\n",
        "    try:\n",
        "        seq_acc_val = sequence_level_accuracy(all_preds, all_labels, test_label_seqs)\n",
        "        print(f\"Sequence Level Accuracy: {seq_acc_val:.4f}\")\n",
        "    except Exception as e:\n",
        "        seq_acc_val = None\n",
        "        print(\"Sequence Level Accuracy could not be computed:\", e)\n",
        "\n",
        "    return avg_loss, acc, mcc, seq_acc_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5623585a",
      "metadata": {
        "id": "5623585a"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "writer = SummaryWriter(log_dir=\"/Users/jonas/Desktop/Uni/PBL/logs/prot_bert_linear_classifier_v6\")\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Freeze encoder parameters initially\n",
        "for param in model.encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.encoder.gradient_checkpointing_enable()  # Enable gradient checkpointing for memory efficienc\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    # Gradually unfreeze subsets of encoder layers for efficiency\n",
        "    if epoch == 4:\n",
        "        try:\n",
        "            for param in model.encoder.encoder.layer[-4:].parameters():\n",
        "                param.requires_grad = True\n",
        "            print(\"Unfreezing last 4 layers of encoder\")\n",
        "        except RuntimeError as e:\n",
        "            print(\"Error unfreezing parameters in Epoch 4:\", e)\n",
        "            torch.mps.empty_cache() # Clear cache immediately on error\n",
        "            continue\n",
        "\n",
        "    elif epoch == 7:\n",
        "        try:\n",
        "            for param in model.encoder.encoder.layer[-7:].parameters():\n",
        "                param.requires_grad = True\n",
        "            print(\"Unfreezing last 7 layers of encoder\")\n",
        "        except RuntimeError as e:\n",
        "            print(\"Error unfreezing parameters in Epoch 7:\", e)\n",
        "            torch.mps.empty_cache() # Clear cache immediately on error\n",
        "            continue\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\")\n",
        "    total_loss = 0  # total epoch loss\n",
        "\n",
        "    for batch in pbar:\n",
        "        try:\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            token_labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()  # reset gradients\n",
        "            with autocast(device_type=DEVICE):  # mixed precision training\n",
        "                loss = model(input_ids, attention_mask, token_labels)  # forward pass\n",
        "\n",
        "            loss = model(input_ids, attention_mask, token_labels)  # forward pass\n",
        "            scaler.scale(loss).backward()      # backpropagation\n",
        "            scaler.unscale_(optimizer) # Unscale gradients before clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # gradient clipping\n",
        "            scaler.step(optimizer)     # update weights\n",
        "            scaler.update()           # update scaler\n",
        "            scheduler.step()     # update learning rate\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        except RuntimeError as e:\n",
        "                print(\"Error during training:\", e)\n",
        "                torch.mps.empty_cache()\n",
        "                continue\n",
        "\n",
        "    torch.mps.empty_cache()  # Clear cache at the end of each epoch\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_train_loss:.4f}\")\n",
        "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
        "\n",
        "writer.flush()\n",
        "writer.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909ee4fa",
      "metadata": {
        "id": "909ee4fa"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.save(model.state_dict(), \"/Users/jonas/Desktop/Uni/PBL/models/prot_bert_linear_classifier_v6.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6707d9f7",
      "metadata": {
        "id": "6707d9f7"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "model = SPCNNClassifier(encoder, NUM_CLASSES).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"/Users/jonas/Desktop/Uni/PBL/models/prot_bert_linear_classifier_v6.pt\", map_location=DEVICE))\n",
        "\n",
        "\n",
        "val_loss = 0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "        # Compute loss using CRF (pass labels)\n",
        "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        # Decode predictions using CRF (no labels passed)\n",
        "        predictions = model(input_ids=input_ids, attention_mask=attention_mask)  # List[List[int]]\n",
        "\n",
        "        # Loop through batch and collect valid tokens\n",
        "        for pred_seq, label_seq, mask in zip(predictions, labels, attention_mask):\n",
        "            for pred, true, is_valid in zip(pred_seq, label_seq, mask):\n",
        "                if true.item() != -100 and is_valid.item() == 1:\n",
        "                    all_preds.append(pred)\n",
        "                    all_labels.append(true.item())\n",
        "\n",
        "\n",
        "# Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=list(label_map.keys())))\n",
        "\n",
        "# F1 Score weighted\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "print(f\"F1 Score (weighted): {f1:.4f}\")\n",
        "# F1 Score macro\n",
        "f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
        "\n",
        "# Sequence Level Accuracy\n",
        "seq_acc_val = sequence_level_accuracy(all_preds, all_labels, test_label_seqs)\n",
        "print(f\"Sequence Level Accuracy: {seq_acc_val:.4f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
        "\n",
        "# Token-level Accuracy\n",
        "token_acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Token-level Accuracy: {token_acc:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(label_map.values()))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_map.keys())\n",
        "disp.plot(cmap=\"OrRd\", xticks_rotation=45)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "writer.add_scalar(\"Loss/test\", val_loss)\n",
        "\n",
        "writer.flush()\n",
        "writer.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}