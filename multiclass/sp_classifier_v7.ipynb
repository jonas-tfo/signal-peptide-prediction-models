{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7PcS_mxevO7"
   },
   "outputs": [],
   "source": [
    "#@title 1. Installs, Imports and Main Configuration\n",
    "# Install necessary libraries\n",
    "!pip install transformers==4.38.2 -q\n",
    "!pip install sentencepiece==0.2.0 -q\n",
    "!pip install torch-xla==2.1.0 -q # For TPU support\n",
    "!pip install pytorch-crf==0.7.2 -q\n",
    "!pip install pandas==2.2.2 -q\n",
    "!pip install scikit-learn==1.4.2 -q\n",
    "!pip install tensorboard==2.15.2 -q\n",
    "\n",
    "# General imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn imports for data handling and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# PyTorch and Transformers imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchcrf import CRF\n",
    "from transformers import T5Tokenizer, T5EncoderModel, get_linear_schedule_with_warmup\n",
    "\n",
    "# --- Main Configuration ---\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"Rostlab/ProstT5\"\n",
    "NUM_CLASSES = 6  # num classes for classification ('S', 'T', 'L', 'I', 'M', 'O')\n",
    "\n",
    "# Training Hyperparameters\n",
    "BATCH_SIZE = 32 # Reduced batch size for better memory management\n",
    "EPOCHS = 10\n",
    "MAX_LENGTH = 512 # Max sequence length for tokenizer\n",
    "\n",
    "# Optimizer Hyperparameters\n",
    "CLASSIFIER_LR = 1e-3 # Learning rate for the new layers (classifier head)\n",
    "ENCODER_LR_INITIAL = 0.0 # Initial LR for the transformer encoder (frozen)\n",
    "ENCODER_LR_UNFROZEN = 2e-5 # LR for the encoder when unfrozen\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# --- Device Setup (CPU, GPU, or TPU) ---\n",
    "TPU_AVAILABLE = False\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    # TPU availability check\n",
    "    if xm.xla_device().type == 'xla':\n",
    "        TPU_AVAILABLE = True\n",
    "        DEVICE = xm.xla_device()\n",
    "        print(f\"TPU detected. Using device: {DEVICE}\")\n",
    "except ImportError:\n",
    "    print(\"torch_xla not found. Falling back to GPU/CPU.\")\n",
    "    DEVICE = (\n",
    "        \"cuda\" if torch.cuda.is_available() else\n",
    "        \"mps\" if torch.backends.mps.is_available() else\n",
    "        \"cpu\"\n",
    "    )\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- File Paths ---\n",
    "# Ensure you have your data in the specified Google Drive path\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/PBL Rost/\"\n",
    "DATA_FILE = os.path.join(DRIVE_PATH, \"data/complete_set_unpartitioned.fasta\")\n",
    "MODEL_SAVE_PATH = os.path.join(DRIVE_PATH, \"models/optimized_bert_classifier.pt\")\n",
    "LOG_DIR = os.path.join(DRIVE_PATH, \"logs/\")\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUNPdif8ey45"
   },
   "outputs": [],
   "source": [
    "#@title 2. Data Loading and Preparation\n",
    "def load_and_prepare_data(data_path: str, label_map: dict):\n",
    "    \"\"\"\n",
    "    Loads data from a FASTA file, performs cleaning, balancing, and splitting.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the FASTA file.\n",
    "        label_map (dict): Mapping from character labels to integer indices.\n",
    "\n",
    "    Returns:\n",
    "        tuple: train_sequences, test_sequences, train_labels, test_labels\n",
    "    \"\"\"\n",
    "    # 1. Load data from FASTA file\n",
    "    print(\"Loading data from FASTA file...\")\n",
    "    records = []\n",
    "    with open(data_path, \"r\") as f:\n",
    "        current_record = {}\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                if current_record:\n",
    "                    records.append(current_record)\n",
    "                header = line[1:].strip().split(\"|\")\n",
    "                # Handle cases where the header might not have 3 parts\n",
    "                if len(header) == 3:\n",
    "                    current_record = {\n",
    "                        \"uniprot_ac\": header[0],\n",
    "                        \"kingdom\": header[1],\n",
    "                        \"type\": header[2],\n",
    "                        \"sequence\": \"\",\n",
    "                        \"label\": \"\"\n",
    "                    }\n",
    "                else:\n",
    "                    current_record = {} # Reset if header is malformed\n",
    "            elif current_record: # Ensure we have a record to add to\n",
    "                # This assumes sequence comes before label\n",
    "                if not current_record.get(\"sequence\"):\n",
    "                    current_record[\"sequence\"] = line.strip()\n",
    "                elif not current_record.get(\"label\"):\n",
    "                    current_record[\"label\"] = line.strip()\n",
    "    if current_record:\n",
    "        records.append(current_record)\n",
    "    df_raw = pd.DataFrame(records)\n",
    "    print(f\"Loaded {len(df_raw)} raw records.\")\n",
    "\n",
    "    # 2. Clean data: drop rows with missing values\n",
    "    df_raw.dropna(subset=['sequence', 'label', 'type'], inplace=True)\n",
    "    print(f\"Records after dropping NA: {len(df_raw)}\")\n",
    "\n",
    "    # 3. Filter out records with 'P' in the label (as in original notebook)\n",
    "    df = df_raw[~df_raw[\"label\"].str.contains(\"P\")].copy()\n",
    "    print(f\"Records after filtering 'P' labels: {len(df)}\")\n",
    "\n",
    "    # 4. Balance classes using oversampling\n",
    "    print(\"Balancing classes using oversampling...\")\n",
    "    df_majority = df[df[\"type\"] == \"NO_SP\"]\n",
    "    df_minority = df[df[\"type\"] != \"NO_SP\"]\n",
    "\n",
    "    if not df_minority.empty:\n",
    "        df_minority_upsampled = resample(\n",
    "            df_minority,\n",
    "            replace=True,\n",
    "            n_samples=len(df_majority),\n",
    "            random_state=42\n",
    "        )\n",
    "        df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "    else:\n",
    "        df_balanced = df_majority.copy()\n",
    "\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Total records after oversampling: {len(df_balanced)}\")\n",
    "    print(\"Class distribution after oversampling:\")\n",
    "    print(df_balanced[\"type\"].value_counts())\n",
    "\n",
    "    # 5. Encode labels and prepare lists\n",
    "    df_balanced[\"label_encoded\"] = df_balanced[\"label\"].apply(\n",
    "        lambda x: [label_map[c] for c in x if c in label_map]\n",
    "    )\n",
    "    # Remove rows where the label sequence became empty after mapping\n",
    "    df_final = df_balanced[df_balanced[\"label_encoded\"].map(len) > 0].copy()\n",
    "\n",
    "    sequences = df_final[\"sequence\"].tolist()\n",
    "    label_seqs = df_final[\"label_encoded\"].tolist()\n",
    "    print(f\"Final dataset size: {len(sequences)}\")\n",
    "\n",
    "    # 6. Split into training and testing sets\n",
    "    train_seqs, test_seqs, train_label_seqs, test_label_seqs = train_test_split(\n",
    "        sequences, label_seqs, test_size=0.2, random_state=42, stratify=df_final['type']\n",
    "    )\n",
    "    print(f\"Training set size: {len(train_seqs)}\")\n",
    "    print(f\"Test set size: {len(test_seqs)}\")\n",
    "\n",
    "    return train_seqs, test_seqs, train_label_seqs, test_label_seqs\n",
    "\n",
    "class SPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for protein sequences and their labels.\n",
    "    Correctly pads labels with -100 for Pytorch's CrossEntropyLoss ignore_index.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, label_seqs, tokenizer, max_length):\n",
    "        self.sequences = sequences\n",
    "        self.label_seqs = label_seqs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        labels = self.label_seqs[idx]\n",
    "\n",
    "        # Tokenize the sequence\n",
    "        # We add spaces between amino acids for the T5 tokenizer\n",
    "        spaced_seq = \" \".join(list(seq))\n",
    "        encoded = self.tokenizer(\n",
    "            spaced_seq,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        input_ids = encoded['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Align labels with tokens. Use -100 for special tokens and padding.\n",
    "        # This is the standard ignore_index for PyTorch loss functions.\n",
    "        token_labels = np.full(self.max_length, -100) # Use numpy for easier assignment\n",
    "\n",
    "        # Find the start of the actual sequence (ignoring initial special tokens)\n",
    "        # For T5, the sequence starts right away, but there's a final </S> token.\n",
    "        # We align labels with the tokenized sequence.\n",
    "        # The number of tokens can be different from the number of amino acids.\n",
    "        # A simple 1-to-1 mapping is a good approximation.\n",
    "        # A more robust approach would use token-to-char mapping from the tokenizer if available.\n",
    "        seq_len = min(len(labels), self.max_length -1) # -1 for the end token\n",
    "        token_labels[:seq_len] = labels[:seq_len]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(token_labels, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOL5-8sde58M"
   },
   "outputs": [],
   "source": [
    "#@title 3. Model Definition (ProstT5-CNN-BiLSTM-CRF)\n",
    "class SPCNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A sophisticated model combining a pre-trained encoder with CNN, BiLSTM, and CRF layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder_model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "        # 1D Convolutional layer to extract local features\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=hidden_size, # Keep dimensionality\n",
    "            kernel_size=5,\n",
    "            padding=\"same\" # \"same\" padding ensures output length is same as input\n",
    "        )\n",
    "        # Batch Normalization for the convolution output\n",
    "        self.bn_conv = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        # 3-layer Bidirectional LSTM to capture long-range dependencies\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size // 2, # Halve size because it's bidirectional\n",
    "            num_layers=3,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.1 # Add dropout between LSTM layers\n",
    "        )\n",
    "        # Classifier head to project LSTM output to the number of classes\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        # Conditional Random Field (CRF) layer to model dependencies between labels\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # 1. Get embeddings from the pre-trained encoder\n",
    "        encoder_output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = encoder_output.last_hidden_state  # (batch, seq_len, hidden_size)\n",
    "\n",
    "        # 2. Apply CNN layer\n",
    "        # Transpose for Conv1d: (batch, hidden_size, seq_len)\n",
    "        x_conv_input = hidden_states.transpose(1, 2)\n",
    "        x_conv = self.conv(x_conv_input)\n",
    "        x_conv = self.bn_conv(x_conv)\n",
    "        x_conv = F.relu(x_conv)\n",
    "        # Transpose back: (batch, seq_len, hidden_size)\n",
    "        x_lstm_input = x_conv.transpose(1, 2)\n",
    "\n",
    "        # 3. Apply BiLSTM layer\n",
    "        lstm_out, _ = self.lstm(x_lstm_input)\n",
    "\n",
    "        # 4. Apply Classifier\n",
    "        logits = self.classifier(lstm_out)\n",
    "        logits = self.dropout(logits)\n",
    "\n",
    "        # 5. Use CRF for loss calculation or decoding\n",
    "        # The CRF layer expects a mask of boolean type.\n",
    "        crf_mask = attention_mask.bool()\n",
    "\n",
    "        if labels is not None:\n",
    "            # Calculate CRF log-likelihood loss.\n",
    "            # The CRF layer internally handles the masking, so we don't need to filter labels.\n",
    "            loss = -self.crf(logits, labels, mask=crf_mask, reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            # Decode the best label sequence using the Viterbi algorithm\n",
    "            predictions = self.crf.decode(logits, mask=crf_mask)\n",
    "            return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_j7NEqze-sU"
   },
   "outputs": [],
   "source": [
    "#@title 4. Training and Evaluation Functions\n",
    "def train_one_epoch(model, loader, optimizer, scheduler, device, scaler=None):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model.\n",
    "        loader: The DataLoader for training data.\n",
    "        optimizer: The optimizer.\n",
    "        scheduler: The learning rate scheduler.\n",
    "        device: The device to train on ('cuda', 'cpu', or XLA device).\n",
    "        scaler: GradScaler for mixed-precision training on CUDA.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision for CUDA\n",
    "        if device == 'cuda' and scaler:\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                loss = model(input_ids, attention_mask, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else: # For CPU and TPU\n",
    "            loss = model(input_ids, attention_mask, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            if TPU_AVAILABLE:\n",
    "                # Use xm.optimizer_step on TPU\n",
    "                xm.optimizer_step(optimizer, barrier=True)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model.\n",
    "        loader: The DataLoader for validation data.\n",
    "        device: The device to evaluate on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (validation_loss, all_predictions, all_labels)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    pbar = tqdm(loader, desc=\"Evaluating\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Get loss and predictions\n",
    "            loss = model(input_ids, attention_mask, labels)\n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Collect valid predictions and labels, ignoring padding/-100\n",
    "            for pred_seq, label_seq, mask in zip(predictions, labels, attention_mask.bool()):\n",
    "                # Unpack the list of lists from predictions\n",
    "                flat_preds = [item for sublist in pred_seq for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "                # Align with labels based on the attention mask\n",
    "                active_labels = label_seq[mask]\n",
    "                # Predictions are for the whole sequence, so we need to align them\n",
    "                active_preds = flat_preds[:len(active_labels)]\n",
    "\n",
    "                all_preds.extend(active_preds)\n",
    "                all_labels.extend(active_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(loader), all_preds, all_labels\n",
    "\n",
    "def sequence_level_accuracy(preds_flat, labels_flat, test_label_seqs):\n",
    "    \"\"\"\n",
    "    Computes sequence-level accuracy. A sequence is correct only if all its labels are correct.\n",
    "    \"\"\"\n",
    "    seq_lengths = [len(seq) for seq in test_label_seqs]\n",
    "    correct_sequences = 0\n",
    "    current_idx = 0\n",
    "\n",
    "    if not preds_flat or not labels_flat:\n",
    "        return 0.0\n",
    "\n",
    "    for length in seq_lengths:\n",
    "        pred_seq = preds_flat[current_idx : current_idx + length]\n",
    "        label_seq = labels_flat[current_idx : current_idx + length]\n",
    "        if pred_seq == label_seq:\n",
    "            correct_sequences += 1\n",
    "        current_idx += length\n",
    "\n",
    "    return correct_sequences / len(seq_lengths) if seq_lengths else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07EJ_J4vfDP_"
   },
   "outputs": [],
   "source": [
    "#@title 5. Main Execution: Setup, Train, and Evaluate\n",
    "# --- 1. Setup ---\n",
    "# Label mapping\n",
    "label_map = {'S': 0, 'T': 1, 'L': 2, 'I': 3, 'M': 4, 'O': 5}\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Load and prepare data\n",
    "train_seqs, test_seqs, train_labels, test_labels = load_and_prepare_data(\n",
    "    data_path=DATA_FILE, label_map=label_map\n",
    ")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "encoder = T5EncoderModel.from_pretrained(MODEL_NAME)\n",
    "model = SPCNNClassifier(encoder, NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = SPDataset(train_seqs, train_labels, tokenizer, MAX_LENGTH)\n",
    "test_dataset = SPDataset(test_seqs, test_labels, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2)\n",
    "\n",
    "# --- 2. Optimizer and Scheduler Setup (with Gradual Unfreezing) ---\n",
    "# Freeze the encoder initially\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define parameter groups for different learning rates\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.encoder.parameters(), 'lr': ENCODER_LR_INITIAL},\n",
    "    {'params': model.conv.parameters(), 'lr': CLASSIFIER_LR},\n",
    "    {'params': model.bn_conv.parameters(), 'lr': CLASSIFIER_LR},\n",
    "    {'params': model.lstm.parameters(), 'lr': CLASSIFIER_LR},\n",
    "    {'params': model.classifier.parameters(), 'lr': CLASSIFIER_LR},\n",
    "    {'params': model.crf.parameters(), 'lr': CLASSIFIER_LR}\n",
    "], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Mixed precision scaler for CUDA\n",
    "scaler = torch.amp.GradScaler(\"cuda\") if DEVICE == 'cuda' else None\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "\n",
    "# --- 3. Training Loop ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    # --- Gradual Unfreezing Logic ---\n",
    "    # At epoch 4, unfreeze the last 4 encoder layers and set their learning rate\n",
    "    if epoch == 4:\n",
    "        print(\"Unfreezing last 4 encoder layers...\")\n",
    "        for block in model.encoder.block[-4:]:\n",
    "            for param in block.parameters():\n",
    "                param.requires_grad = True\n",
    "        optimizer.param_groups[0]['lr'] = ENCODER_LR_UNFROZEN\n",
    "\n",
    "    # At epoch 7, unfreeze all remaining encoder layers\n",
    "    if epoch == 7:\n",
    "        print(\"Unfreezing all encoder layers...\")\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        optimizer.param_groups[0]['lr'] = ENCODER_LR_UNFROZEN\n",
    "\n",
    "    # Train for one epoch\n",
    "    avg_train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, DEVICE, scaler)\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    avg_val_loss, preds, labels = evaluate(model, test_loader, DEVICE)\n",
    "    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/validation\", avg_val_loss, epoch)\n",
    "\n",
    "    # Log metrics if predictions were made\n",
    "    if preds and labels:\n",
    "        token_acc = accuracy_score(labels, preds)\n",
    "        writer.add_scalar(\"Accuracy/token\", token_acc, epoch)\n",
    "        print(f\"Token-level Accuracy: {token_acc:.4f}\")\n",
    "\n",
    "# --- 4. Final Evaluation and Saving ---\n",
    "print(\"\\n--- Final Evaluation ---\")\n",
    "val_loss, all_preds, all_labels = evaluate(model, test_loader, DEVICE)\n",
    "\n",
    "if all_preds and all_labels:\n",
    "    # Get metrics\n",
    "    report = classification_report(\n",
    "        all_labels, all_preds,\n",
    "        target_names=list(label_map.keys()),\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    )\n",
    "    seq_acc = sequence_level_accuracy(all_preds, all_labels, test_labels)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(f\"Sequence Level Accuracy: {seq_acc:.4f}\")\n",
    "    print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=list(label_map.values()))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(label_map.keys()))\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp.plot(cmap=\"OrRd\", xticks_rotation=45, ax=ax)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Evaluation produced no predictions. Skipping metrics calculation.\")\n",
    "\n",
    "# Save the model\n",
    "print(f\"Saving model to {MODEL_SAVE_PATH}\")\n",
    "# Ensure directory exists\n",
    "os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "if TPU_AVAILABLE:\n",
    "    # Use xm.save for TPUs\n",
    "    xm.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "else:\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "\n",
    "writer.close()\n",
    "print(\"--- Training and Evaluation Complete ---\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
