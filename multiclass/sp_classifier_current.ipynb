{"cells":[{"cell_type":"code","source":["#@title 0. uv calibration\n","import os\n","!curl -Ls https://astral.sh/uv/install.sh | bash\n","os.environ[\"PATH\"] += \":/root/.cargo/bin\"\n","!uv --version"],"metadata":{"id":"mgrJ8KkUgJ-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7PcS_mxevO7"},"outputs":[],"source":["#@title 1. Installs, Imports and Main Configuration\n","!uv pip install transformers==4.38.2 -q\n","!uv pip install sentencepiece==0.2.0 -q\n","!uv pip install torch-xla==2.1.0 -q # For TPU support\n","!uv pip install pytorch-crf==0.7.2 -q\n","!uv pip install pandas==2.2.2 -q\n","!uv pip install scikit-learn==1.4.2 -q\n","!uv pip install tensorboard==2.15.2 -q\n","\n","# General imports\n","import os\n","import pandas as pd\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","\n","# Scikit-learn imports for data handling and metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","from sklearn.metrics import (\n","    classification_report,\n","    matthews_corrcoef,\n","    accuracy_score,\n","    f1_score,\n","    confusion_matrix,\n","    ConfusionMatrixDisplay\n",")\n","\n","# PyTorch and Transformers imports\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from torchcrf import CRF\n","from transformers import T5Tokenizer, T5EncoderModel, get_linear_schedule_with_warmup\n","import tensorflow as tf\n","try:\n","  resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n","  tf.config.experimental_connect_to_cluster(resolver)\n","  tf.tpu.experimental.initialize_tpu_system(resolver)\n","  print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n","  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n","except ValueError:\n","  strategy = tf.distribute.get_strategy()\n","\n","# --- Main Configuration ---\n","\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","# Model Configuration\n","MODEL_NAME = \"Rostlab/ProstT5\"\n","NUM_CLASSES = 6  # num classes for classification ('S', 'T', 'L', 'I', 'M', 'O')\n","\n","# Training Hyperparameters\n","BATCH_SIZE = 16 # Reduced batch size for better memory management\n","EPOCHS = 10\n","MAX_LENGTH = 512 # Max sequence length for tokenizer\n","\n","# Optimizer Hyperparameters\n","CLASSIFIER_LR = 1e-3 # Learning rate for the new layers (classifier head)\n","ENCODER_LR_INITIAL = 0.0 # Initial LR for the transformer encoder (frozen)\n","ENCODER_LR_UNFROZEN = 2e-5 # LR for the encoder when unfrozen\n","WEIGHT_DECAY = 0.01\n","\n","# --- Device Setup (CPU, GPU, or TPU) ---\n","TPU_AVAILABLE = False\n","try:\n","    import torch_xla.core.xla_model as xm\n","    TPU_AVAILABLE = xm.xla_device() == 'xla'\n","except ImportError:\n","    TPU_AVAILABLE = False\n","\n","DEVICE = (\n","    \"xla\" if TPU_AVAILABLE else\n","    \"mps\" if torch.backends.mps.is_available() else\n","    \"cuda\" if torch.cuda.is_available() else\n","    \"cpu\"\n",")\n","print(f\"Using device: {DEVICE}\")\n","\n","# --- File Paths ---\n","# Ensure you have your data in the specified Google Drive path\n","DRIVE_PATH = \"/content/drive/MyDrive/PBL Rost/\"\n","DATA_FILE = os.path.join(DRIVE_PATH, \"data/complete_set_unpartitioned.fasta\")\n","MODEL_SAVE_PATH = os.path.join(DRIVE_PATH, \"models/optimized_bert_classifier.pt\")\n","LOG_DIR = os.path.join(DRIVE_PATH, \"logs/\")\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUNPdif8ey45"},"outputs":[],"source":["#@title 2. Data Loading and Preparation\n","def load_and_prepare_data(data_path: str, label_map: dict):\n","    \"\"\"\n","    Loads data from a FASTA file, performs cleaning, balancing, and splitting.\n","\n","    Args:\n","        data_path (str): Path to the FASTA file.\n","        label_map (dict): Mapping from character labels to integer indices.\n","\n","    Returns:\n","        tuple: train_sequences, test_sequences, train_labels, test_labels\n","    \"\"\"\n","    # 1. Load data from FASTA file\n","    print(\"Loading data from FASTA file...\")\n","    records = []\n","    with open(data_path, \"r\") as f:\n","        current_record = {}\n","        for line in f:\n","            if line.startswith(\">\"):\n","                if current_record:\n","                    records.append(current_record)\n","                header = line[1:].strip().split(\"|\")\n","                # Handle cases where the header might not have 3 parts\n","                if len(header) == 3:\n","                    current_record = {\n","                        \"uniprot_ac\": header[0],\n","                        \"kingdom\": header[1],\n","                        \"type\": header[2],\n","                        \"sequence\": \"\",\n","                        \"label\": \"\"\n","                    }\n","                else:\n","                    current_record = {} # Reset if header is malformed\n","            elif current_record: # Ensure we have a record to add to\n","                # This assumes sequence comes before label\n","                if not current_record.get(\"sequence\"):\n","                    current_record[\"sequence\"] = line.strip()\n","                elif not current_record.get(\"label\"):\n","                    current_record[\"label\"] = line.strip()\n","    if current_record:\n","        records.append(current_record)\n","    df_raw = pd.DataFrame(records)\n","    print(f\"Loaded {len(df_raw)} raw records.\")\n","\n","    # 2. Clean data: drop rows with missing values\n","    df_raw.dropna(subset=['sequence', 'label', 'type'], inplace=True)\n","    print(f\"Records after dropping NA: {len(df_raw)}\")\n","\n","    # 3. Filter out records with 'P' in the label (as in original notebook)\n","    df = df_raw[~df_raw[\"label\"].str.contains(\"P\")].copy()\n","    print(f\"Records after filtering 'P' labels: {len(df)}\")\n","\n","    # 4. Balance classes using oversampling\n","    print(\"Balancing classes using oversampling...\")\n","    df_majority = df[df[\"type\"] == \"NO_SP\"]\n","    df_minority = df[df[\"type\"] != \"NO_SP\"]\n","\n","    if not df_minority.empty:\n","        df_minority_upsampled = resample(\n","            df_minority,\n","            replace=True,\n","            n_samples=len(df_majority),\n","            random_state=42\n","        )\n","        df_balanced = pd.concat([df_majority, df_minority_upsampled])\n","    else:\n","        df_balanced = df_majority.copy()\n","\n","    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n","    print(f\"Total records after oversampling: {len(df_balanced)}\")\n","    print(\"Class distribution after oversampling:\")\n","    print(df_balanced[\"type\"].value_counts())\n","\n","    # 5. Encode labels and prepare lists\n","    # Ensure labels are within the valid range before encoding\n","    valid_chars = list(label_map.keys())\n","    df_balanced[\"label_encoded\"] = df_balanced[\"label\"].apply(\n","        lambda x: [label_map[c] for c in x if c in valid_chars]\n","    )\n","    # Remove rows where the label sequence became empty after mapping\n","    df_final = df_balanced[df_balanced[\"label_encoded\"].map(len) > 0].copy()\n","\n","    sequences = df_final[\"sequence\"].tolist()\n","    label_seqs = df_final[\"label_encoded\"].tolist()\n","    print(f\"Final dataset size: {len(sequences)}\")\n","\n","    # 6. Split into training and testing sets\n","    train_seqs, test_seqs, train_label_seqs, test_label_seqs = train_test_split(\n","        sequences, label_seqs, test_size=0.2, random_state=42, stratify=df_final['type']\n","    )\n","    print(f\"Training set size: {len(train_seqs)}\")\n","    print(f\"Test set size: {len(test_seqs)}\")\n","\n","    return train_seqs, test_seqs, train_label_seqs, test_label_seqs\n","\n","class SPDataset(Dataset):\n","    \"\"\"\n","    Custom PyTorch Dataset for protein sequences and their labels.\n","    Correctly pads labels with -100 for Pytorch's CrossEntropyLoss ignore_index.\n","    \"\"\"\n","    def __init__(self, sequences, label_seqs, tokenizer, max_length):\n","        self.sequences = sequences\n","        self.label_seqs = label_seqs\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        seq = self.sequences[idx]\n","        labels = self.label_seqs[idx]\n","\n","        # Tokenize the sequence\n","        # We add spaces between amino acids for the T5 tokenizer\n","        spaced_seq = \" \".join(list(seq))\n","        encoded = self.tokenizer(\n","            spaced_seq,\n","            return_tensors=\"pt\",\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=self.max_length\n","        )\n","        input_ids = encoded['input_ids'].squeeze(0)\n","        attention_mask = encoded['attention_mask'].squeeze(0)\n","\n","        # add 0 padding\n","        token_labels = np.zeros(self.max_length, dtype=int)\n","\n","        # Get the length of the actual tokenized sequence (excluding padding)\n","        # This is the sum of the attention mask\n","        actual_token_len = attention_mask.sum().item()\n","\n","        # Determine the length of the labels to use based on the actual token length\n","        # and the original label sequence length, capped by max_length - 1 (for </S>)\n","        # The T5 tokenizer adds a </S> token at the end. We should not have a label for it.\n","        # The labels correspond to the amino acid sequence, which aligns with the input tokens\n","        # before the final </S> token and any padding.\n","        label_len_to_use = min(len(labels), actual_token_len -1) # -1 for the end token\n","\n","        # Place the actual labels at the beginning, corresponding to the non-padded, non-</S> tokens\n","        if label_len_to_use > 0:\n","             # Ensure labels are within the valid range [0, NUM_CLASSES-1]\n","            valid_labels = [l for l in labels[:label_len_to_use] if 0 <= l < len(label_map)]\n","            # If filtering removed any labels, adjust the length\n","            label_len_to_use = len(valid_labels)\n","            token_labels[:label_len_to_use] = torch.tensor(valid_labels, dtype=torch.long)\n","\n","\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': token_labels\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LOL5-8sde58M"},"outputs":[],"source":["#@title 3. Model Definition (ProstT5-CNN-BiLSTM-CRF)\n","class SPCNNClassifier(nn.Module):\n","    \"\"\"\n","    A sophisticated model combining a pre-trained encoder with CNN, BiLSTM, and CRF layers.\n","    \"\"\"\n","    def __init__(self, encoder_model, num_labels):\n","        super().__init__()\n","        self.encoder = encoder_model\n","        self.dropout = nn.Dropout(0.2)\n","        hidden_size = self.encoder.config.hidden_size\n","\n","        # 1D Convolutional layer to extract local features\n","        self.conv = nn.Conv1d(\n","            in_channels=hidden_size,\n","            out_channels=hidden_size, # Keep dimensionality\n","            kernel_size=5,\n","            padding=\"same\" # \"same\" padding ensures output length is same as input\n","        )\n","        # Batch Normalization for the convolution output\n","        self.bn_conv = nn.BatchNorm1d(hidden_size)\n","\n","        # 3-layer Bidirectional LSTM to capture long-range dependencies\n","        self.lstm = nn.LSTM(\n","            input_size=hidden_size,\n","            hidden_size=hidden_size // 2, # Halve size because it's bidirectional\n","            num_layers=3,\n","            bidirectional=True,\n","            batch_first=True,\n","            dropout=0.1 # Add dropout between LSTM layers\n","        )\n","        # Classifier head to project LSTM output to the number of classes\n","        self.classifier = nn.Linear(hidden_size, num_labels)\n","        # Conditional Random Field (CRF) layer to model dependencies between labels\n","        self.crf = CRF(num_labels, batch_first=True)\n","\n","    def forward(self, input_ids, attention_mask, labels=None, return_loss_only=False):\n","        # 1. Get embeddings from the pre-trained encoder\n","        encoder_output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_states = encoder_output.last_hidden_state  # (batch, seq_len, hidden_size)\n","\n","        # 2. Apply CNN layer\n","        # Transpose for Conv1d: (batch, hidden_size, seq_len)\n","        x_conv_input = hidden_states.transpose(1, 2)\n","        x_conv = self.conv(x_conv_input)\n","        x_conv = self.bn_conv(x_conv)\n","        x_conv = F.relu(x_conv)\n","        # Transpose back: (batch, seq_len, hidden_size)\n","        x_lstm_input = x_conv.transpose(1, 2)\n","\n","        # 3. Apply BiLSTM layer\n","        lstm_out, _ = self.lstm(x_lstm_input)\n","\n","        # 4. Apply Classifier\n","        logits = self.classifier(lstm_out)\n","        logits = self.dropout(logits)\n","\n","        # 5. Use CRF for loss calculation or decoding\n","        # The CRF layer expects a mask of boolean type.\n","        crf_mask = attention_mask.bool()\n","\n","        if labels is not None:\n","            loss = -self.crf(logits, labels, mask=crf_mask, reduction='mean')\n","            if return_loss_only:\n","                return loss\n","            else:\n","                predictions = self.crf.decode(logits, mask=crf_mask)\n","                return loss, predictions\n","        else:\n","            predictions = self.crf.decode(logits, mask=crf_mask)\n","            return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_j7NEqze-sU"},"outputs":[],"source":["#@title 4. Training and Evaluation Functions\n","def train_one_epoch(model, loader, optimizer, scheduler, device, scaler=None):\n","    \"\"\"\n","    Trains the model for one epoch.\n","\n","    Args:\n","        model: The PyTorch model.\n","        loader: The DataLoader for training data.\n","        optimizer: The optimizer.\n","        scheduler: The learning rate scheduler.\n","        device: The device to train on ('cuda', 'cpu', or XLA device).\n","        scaler: GradScaler for mixed-precision training on CUDA.\n","    \"\"\"\n","    model.train()\n","    total_loss = 0\n","    pbar = tqdm(loader, desc=\"Training\", leave=False)\n","\n","    for batch in pbar:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Get loss only for backpropagation\n","        loss = model(input_ids, attention_mask, labels, return_loss_only=True)\n","\n","        if scaler:\n","            scaler.scale(loss).backward()\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            scaler.step(optimizer)\n","            scaler.update()\n","        else:\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            if TPU_AVAILABLE:\n","                # Use xm.optimizer_step on TPU\n","                xm.optimizer_step(optimizer, barrier=True)\n","            else:\n","                optimizer.step()\n","\n","        scheduler.step()\n","        total_loss += loss.item()\n","        pbar.set_postfix(loss=f'{loss.item():.4f}')\n","\n","    return total_loss / len(loader)\n","\n","def evaluate(model, loader, device):\n","    \"\"\"\n","    Evaluates the model on the validation set.\n","\n","    Args:\n","        model: The PyTorch model.\n","        loader: The DataLoader for validation data.\n","        device: The device to evaluate on.\n","\n","    Returns:\n","        tuple: (validation_loss, all_predictions, all_labels)\n","    \"\"\"\n","    model.eval()\n","    total_loss = 0\n","    all_preds, all_labels = [], []\n","    pbar = tqdm(loader, desc=\"Evaluating\", leave=False)\n","\n","    with torch.no_grad():\n","        for batch in pbar:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            # Get loss and predictions (loss is calculated even in eval mode when labels are provided)\n","            loss, predictions = model(input_ids, attention_mask, labels, return_loss_only=False) # Keep return_loss_only=False for evaluation\n","\n","            total_loss += loss.item()\n","\n","            # Collect valid predictions and labels, ignoring padding/-100\n","            for pred_seq, label_seq, mask in zip(predictions, labels, attention_mask.bool()):\n","                # Unpack the list of lists from predictions\n","                # predictions from CRF decode is a list of lists, where each inner list is a sequence of predicted tags\n","                flat_preds = [item for sublist in pred_seq for item in (sublist if isinstance(sublist, list) else [sublist])]\n","\n","\n","                # Align with labels based on the attention mask\n","                # and ignore labels that are -100 (padding in the custom dataset)\n","                active_labels = label_seq[mask]\n","                # Exclude -100 labels and corresponding predictions\n","                valid_indices = (active_labels != -100).nonzero(as_tuple=True)[0]\n","                valid_labels = active_labels[valid_indices]\n","                # Predictions are for the whole sequence, so we need to align them with the valid labels\n","                valid_preds = [flat_preds[i] for i in valid_indices]\n","\n","\n","                all_preds.extend(valid_preds)\n","                all_labels.extend(valid_labels.cpu().numpy())\n","\n","    return total_loss / len(loader), all_preds, all_labels\n","\n","def sequence_level_accuracy(preds_flat, labels_flat, test_label_seqs):\n","    \"\"\"\n","    Computes sequence-level accuracy. A sequence is correct only if all its labels are correct.\n","    Note: This function now relies on the flat lists and will need the original sequence lengths\n","          to correctly group predictions and labels.\n","    \"\"\"\n","    # The original test_label_seqs provides the lengths before padding and filtering -100.\n","    # We need to reconstruct the sequences from the flat lists based on these original lengths\n","    # and the valid labels/preds that were collected.\n","\n","    correct_sequences = 0\n","    current_flat_idx = 0\n","\n","    if not preds_flat or not labels_flat:\n","        return 0.0\n","\n","    for original_seq_labels in test_label_seqs:\n","        # Determine the number of valid labels for this original sequence from the flat list\n","        # This assumes the order in the flat lists matches the order of sequences in test_label_seqs\n","        num_valid_labels_in_flat = sum(1 for label in original_seq_labels if 0 <= label < len(label_map)) # Count non -100 like labels based on original map size\n","\n","        # Extract the corresponding chunk from the flat lists\n","        pred_seq_flat_chunk = preds_flat[current_flat_idx : current_flat_idx + num_valid_labels_in_flat]\n","        label_seq_flat_chunk = labels_flat[current_flat_idx : current_flat_idx + num_valid_labels_in_flat]\n","\n","        if pred_seq_flat_chunk == list(label_seq_flat_chunk): # Convert numpy array to list for comparison\n","             correct_sequences += 1\n","\n","        current_flat_idx += num_valid_labels_in_flat\n","\n","\n","    return correct_sequences / len(test_label_seqs) if test_label_seqs else 0.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"07EJ_J4vfDP_"},"outputs":[],"source":["#@title 5. Main Execution: Setup, Train, and Evaluate\n","# --- 1. Setup ---\n","# Label mapping\n","label_map = {'S': 0, 'T': 1, 'L': 2, 'I': 3, 'M': 4, 'O': 5}\n","inv_label_map = {v: k for k, v in label_map.items()}\n","\n","# Load and prepare data\n","train_seqs, test_seqs, train_labels, test_labels = load_and_prepare_data(\n","    data_path=DATA_FILE, label_map=label_map\n",")\n","\n","# Initialize tokenizer and model\n","tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n","encoder = T5EncoderModel.from_pretrained(MODEL_NAME)\n","model = SPCNNClassifier(encoder, NUM_CLASSES).to(DEVICE)\n","\n","# Create Datasets and DataLoaders\n","train_dataset = SPDataset(train_seqs, train_labels, tokenizer, MAX_LENGTH)\n","test_dataset = SPDataset(test_seqs, test_labels, tokenizer, MAX_LENGTH)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2)\n","\n","# --- 2. Optimizer and Scheduler Setup (with Gradual Unfreezing) ---\n","# Freeze the encoder initially\n","for param in model.encoder.parameters():\n","    param.requires_grad = False\n","\n","# Define parameter groups for different learning rates\n","optimizer = torch.optim.AdamW([\n","    {'params': model.encoder.parameters(), 'lr': ENCODER_LR_INITIAL},\n","    {'params': model.conv.parameters(), 'lr': CLASSIFIER_LR},\n","    {'params': model.bn_conv.parameters(), 'lr': CLASSIFIER_LR},\n","    {'params': model.lstm.parameters(), 'lr': CLASSIFIER_LR},\n","    {'params': model.classifier.parameters(), 'lr': CLASSIFIER_LR},\n","    {'params': model.crf.parameters(), 'lr': CLASSIFIER_LR}\n","], weight_decay=WEIGHT_DECAY)\n","\n","# Learning rate scheduler\n","total_steps = len(train_loader) * EPOCHS\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=int(0.1 * total_steps),\n","    num_training_steps=total_steps\n",")\n","\n","# Mixed precision scaler for CUDA\n","scaler = torch.amp.GradScaler(\"cuda\") if DEVICE == 'cuda' else None\n","\n","# TensorBoard writer\n","writer = SummaryWriter(log_dir=LOG_DIR)\n","\n","\n","# --- 3. Training Loop ---\n","print(\"\\n--- Starting Training ---\")\n","for epoch in range(EPOCHS):\n","    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n","\n","    # --- Gradual Unfreezing Logic ---\n","    # At epoch 4, unfreeze the last 4 encoder layers and set their learning rate\n","    if epoch == 4:\n","        print(\"Unfreezing last 4 encoder layers...\")\n","        for block in model.encoder.block[-4:]:\n","            for param in block.parameters():\n","                param.requires_grad = True\n","        optimizer.param_groups[0]['lr'] = ENCODER_LR_UNFROZEN\n","\n","    # At epoch 7, unfreeze all remaining encoder layers\n","    if epoch == 7:\n","        print(\"Unfreezing all encoder layers...\")\n","        for param in model.encoder.parameters():\n","            param.requires_grad = True\n","        optimizer.param_groups[0]['lr'] = ENCODER_LR_UNFROZEN\n","\n","    # Train for one epoch\n","    avg_train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, DEVICE, scaler)\n","    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n","    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n","\n","    # Evaluate on the test set\n","    avg_val_loss, preds, labels = evaluate(model, test_loader, DEVICE)\n","    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n","    writer.add_scalar(\"Loss/validation\", avg_val_loss, epoch)\n","\n","    # Log metrics if predictions were made\n","    if preds and labels:\n","        token_acc = accuracy_score(labels, preds)\n","        writer.add_scalar(\"Accuracy/token\", token_acc, epoch)\n","        print(f\"Token-level Accuracy: {token_acc:.4f}\")\n","\n","# --- 4. Final Evaluation and Saving ---\n","print(\"\\n--- Final Evaluation ---\")\n","val_loss, all_preds, all_labels = evaluate(model, test_loader, DEVICE)\n","\n","if all_preds and all_labels:\n","    # Get metrics\n","    report = classification_report(\n","        all_labels, all_preds,\n","        target_names=list(label_map.keys()),\n","        digits=4,\n","        zero_division=0\n","    )\n","    seq_acc = sequence_level_accuracy(all_preds, all_labels, test_labels)\n","    mcc = matthews_corrcoef(all_labels, all_preds)\n","    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n","\n","    print(\"Classification Report:\")\n","    print(report)\n","    print(f\"Sequence Level Accuracy: {seq_acc:.4f}\")\n","    print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n","    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n","\n","    # Plot confusion matrix\n","    cm = confusion_matrix(all_labels, all_preds, labels=list(label_map.values()))\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(label_map.keys()))\n","    fig, ax = plt.subplots(figsize=(8, 8))\n","    disp.plot(cmap=\"OrRd\", xticks_rotation=45, ax=ax)\n","    plt.title(\"Confusion Matrix\")\n","    plt.show()\n","else:\n","    print(\"Evaluation produced no predictions. Skipping metrics calculation.\")\n","\n","# Save the model\n","print(f\"Saving model to {MODEL_SAVE_PATH}\")\n","# Ensure directory exists\n","os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n","if TPU_AVAILABLE:\n","    # Use xm.save for TPUs\n","    xm.save(model.state_dict(), MODEL_SAVE_PATH)\n","else:\n","    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n","\n","writer.close()\n","print(\"--- Training and Evaluation Complete ---\")\n","\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/jonas-tfo/signal-peptide-prediction-models/blob/main/multiclass/sp_classifier_v7.ipynb","timestamp":1750670108382}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}