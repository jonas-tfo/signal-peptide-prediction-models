{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgrJ8KkUgJ-J"
   },
   "outputs": [],
   "source": [
    "#@title 0. uv calibration\n",
    "import os\n",
    "!curl -Ls https://astral.sh/uv/install.sh | bash\n",
    "os.environ[\"PATH\"] += \":/root/.cargo/bin\"\n",
    "!uv --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7PcS_mxevO7"
   },
   "outputs": [],
   "source": [
    "#@title 1. Installs, Imports and Main Configuration\n",
    "!uv pip install transformers==4.38.2 -q\n",
    "!uv pip install sentencepiece==0.2.0 -q\n",
    "!uv pip install torch-xla==2.1.0 -q # For TPU support\n",
    "!uv pip install pytorch-crf==0.7.2 -q\n",
    "!uv pip install pandas==2.2.2 -q\n",
    "!uv pip install scikit-learn==1.4.2 -q\n",
    "!uv pip install tensorboard==2.15.2 -q\n",
    "!uv pip install flaxcrf -U\n",
    "\n",
    "\n",
    "# General imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn imports for data handling and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# check if tpu is available\n",
    "import torch\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    TPU_AVAILABLE = xm.xla_device() == 'xla'\n",
    "except ImportError:\n",
    "    TPU_AVAILABLE = False\n",
    "\n",
    "# Jax imports for TPU support\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flaxcrf import CRF\n",
    "import optax\n",
    "from transformers import FlaxT5ForConditionalGeneration, AutoTokenizer\n",
    "from typing import Sequence, Tuple, Any, Optional\n",
    "\n",
    "# --- Main Configuration ---\n",
    "\n",
    "MODEL_NAME = \"Rostlab/ProstT5\"\n",
    "NUM_CLASSES = 6  # num classes for classification ('S', 'T', 'L', 'I', 'M', 'O')\n",
    "LABEL_MAP = {'S': 0, 'T': 1, 'L': 2, 'I': 3, 'M': 4, 'O': 5}\n",
    "\n",
    "# Training Hyperparameters\n",
    "BATCH_SIZE = 16 # Reduced batch size for better memory management\n",
    "EPOCHS = 10\n",
    "MAX_LENGTH = 512 # Max sequence length for tokenizer\n",
    "\n",
    "# Optimizer Hyperparameters\n",
    "CLASSIFIER_LR = 1e-3 # Learning rate for the new layers (classifier head)\n",
    "ENCODER_LR_INITIAL = 0.0 # Initial LR for the transformer encoder (frozen)\n",
    "ENCODER_LR_UNFROZEN = 2e-5 # LR for the encoder when unfrozen\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# --- Device Setup (CPU, GPU, or TPU) ---\n",
    "TPU_AVAILABLE = False\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    TPU_AVAILABLE = xm.xla_device() == 'xla'\n",
    "except ImportError:\n",
    "    TPU_AVAILABLE = False\n",
    "\n",
    "DEVICE = (\n",
    "    \"xla\" if TPU_AVAILABLE else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- File Paths ---\n",
    "# Ensure you have your data in the specified Google Drive path\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/PBL Rost/\"\n",
    "DATA_FILE = os.path.join(DRIVE_PATH, \"data/complete_set_unpartitioned.fasta\")\n",
    "MODEL_SAVE_PATH = os.path.join(DRIVE_PATH, \"models/optimized_bert_classifier.pt\")\n",
    "LOG_DIR = os.path.join(DRIVE_PATH, \"logs/\")\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer and encoder\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "ENCODER = FlaxT5ForConditionalGeneration.from_pretrained(MODEL_NAME).encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUNPdif8ey45"
   },
   "outputs": [],
   "source": [
    "#@title 2. Data Loading and Preparation\n",
    "def load_and_prepare_data(data_path: str, label_map: dict):\n",
    "    \"\"\"\n",
    "    Loads data from a FASTA file, performs cleaning, balancing, and splitting.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the FASTA file.\n",
    "        label_map (dict): Mapping from character labels to integer indices.\n",
    "\n",
    "    Returns:\n",
    "        tuple: train_sequences, test_sequences, train_labels, test_labels\n",
    "    \"\"\"\n",
    "    # 1. Load data from FASTA file\n",
    "    print(\"Loading data from FASTA file...\")\n",
    "    records = []\n",
    "    with open(data_path, \"r\") as f:\n",
    "        current_record = {}\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                if current_record:\n",
    "                    records.append(current_record)\n",
    "                header = line[1:].strip().split(\"|\")\n",
    "                # Handle cases where the header might not have 3 parts\n",
    "                if len(header) == 3:\n",
    "                    current_record = {\n",
    "                        \"uniprot_ac\": header[0],\n",
    "                        \"kingdom\": header[1],\n",
    "                        \"type\": header[2],\n",
    "                        \"sequence\": \"\",\n",
    "                        \"label\": \"\"\n",
    "                    }\n",
    "                else:\n",
    "                    current_record = {} # Reset if header is malformed\n",
    "            elif current_record: # Ensure we have a record to add to\n",
    "                # This assumes sequence comes before label\n",
    "                if not current_record.get(\"sequence\"):\n",
    "                    current_record[\"sequence\"] = line.strip()\n",
    "                elif not current_record.get(\"label\"):\n",
    "                    current_record[\"label\"] = line.strip()\n",
    "    if current_record:\n",
    "        records.append(current_record)\n",
    "    df_raw = pd.DataFrame(records)\n",
    "    print(f\"Loaded {len(df_raw)} raw records.\")\n",
    "\n",
    "    # 2. Clean data: drop rows with missing values\n",
    "    df_raw.dropna(subset=['sequence', 'label', 'type'], inplace=True)\n",
    "    print(f\"Records after dropping NA: {len(df_raw)}\")\n",
    "\n",
    "    # 3. Filter out records with 'P' in the label (as in original notebook)\n",
    "    df = df_raw[~df_raw[\"label\"].str.contains(\"P\")].copy()\n",
    "    print(f\"Records after filtering 'P' labels: {len(df)}\")\n",
    "\n",
    "    # 4. Balance classes using oversampling\n",
    "    print(\"Balancing classes using oversampling...\")\n",
    "    df_majority = df[df[\"type\"] == \"NO_SP\"]\n",
    "    df_minority = df[df[\"type\"] != \"NO_SP\"]\n",
    "\n",
    "    if not df_minority.empty:\n",
    "        df_minority_upsampled = resample(\n",
    "            df_minority,\n",
    "            replace=True,\n",
    "            n_samples=len(df_majority),\n",
    "            random_state=42\n",
    "        )\n",
    "        df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "    else:\n",
    "        df_balanced = df_majority.copy()\n",
    "\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Total records after oversampling: {len(df_balanced)}\")\n",
    "    print(\"Class distribution after oversampling:\")\n",
    "    print(df_balanced[\"type\"].value_counts())\n",
    "\n",
    "    # 5. Encode labels and prepare lists\n",
    "    # Ensure labels are within the valid range before encoding\n",
    "    valid_chars = list(label_map.keys())\n",
    "    df_balanced[\"label_encoded\"] = df_balanced[\"label\"].apply(\n",
    "        lambda x: [label_map[c] for c in x if c in valid_chars]\n",
    "    )\n",
    "    # Remove rows where the label sequence became empty after mapping\n",
    "    df_final = df_balanced[df_balanced[\"label_encoded\"].map(len) > 0].copy()\n",
    "\n",
    "    sequences = df_final[\"sequence\"].tolist()\n",
    "    label_seqs = df_final[\"label_encoded\"].tolist()\n",
    "    print(f\"Final dataset size: {len(sequences)}\")\n",
    "\n",
    "    # 6. Split into training and testing sets\n",
    "    train_seqs, test_seqs, train_label_seqs, test_label_seqs = train_test_split(\n",
    "        sequences, label_seqs, test_size=0.2, random_state=42, stratify=df_final['type']\n",
    "    )\n",
    "    print(f\"Training set size: {len(train_seqs)}\")\n",
    "    print(f\"Test set size: {len(test_seqs)}\")\n",
    "\n",
    "    return train_seqs, test_seqs, train_label_seqs, test_label_seqs\n",
    "\n",
    "\n",
    "\n",
    "# Tokenization and Batching\n",
    "def tokenize_and_batch(sequences, labels, tokenizer, max_length, batch_size):\n",
    "    \"\"\"\n",
    "    Tokenizes sequences and creates batches for training.\n",
    "    Returns a list of dicts with input_ids, attention_mask, and labels.\n",
    "    \"\"\"\n",
    "    # Tokenize all sequences\n",
    "    encodings = tokenizer(\n",
    "        sequences,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='np'  # using numpy for JAX\n",
    "    )\n",
    "    # Pad/truncate labels to max_length\n",
    "    padded_labels = [\n",
    "        np.pad(l, (0, max_length - len(l)), constant_values=-100)[:max_length]\n",
    "        for l in labels\n",
    "    ]\n",
    "    # Create batches\n",
    "    input_ids = encodings['input_ids']\n",
    "    attention_mask = encodings['attention_mask']\n",
    "    labels = np.array(padded_labels)\n",
    "    num_samples = len(sequences)\n",
    "    batches = []\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch = {\n",
    "            'input_ids': jnp.array(input_ids[i:i+batch_size]),\n",
    "            'attention_mask': jnp.array(attention_mask[i:i+batch_size]),\n",
    "            'labels': jnp.array(labels[i:i+batch_size])\n",
    "        }\n",
    "        batches.append(batch)\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Tokenization and Batching for preparation for training\n",
    "\n",
    "# fasta file reading and split\n",
    "train_seqs, test_seqs, train_label_seqs, test_label_seqs = load_and_prepare_data(DATA_FILE, LABEL_MAP)\n",
    "\n",
    "# Example usage after loading data\n",
    "train_batches = tokenize_and_batch(train_seqs, train_label_seqs, TOKENIZER, MAX_LENGTH, BATCH_SIZE)\n",
    "test_batches = tokenize_and_batch(test_seqs, test_label_seqs, TOKENIZER, MAX_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOL5-8sde58M"
   },
   "outputs": [],
   "source": [
    "class SPCNNClassifier(nn.Module):\n",
    "    encoder: nn.Module  # encoder as a submodule\n",
    "    num_labels: int\n",
    "    dropout_rate: float = 0.2\n",
    "    lstm_dropout_rate: float = 0.1\n",
    "    kernel_size: int = 5\n",
    "    num_lstm_layers: int = 3\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, input_ids, attention_mask, labels=None, training=False):\n",
    "        # Use the encoder submodule\n",
    "        encoder_output = self.encoder.encode(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            train=training\n",
    "        )\n",
    "        hidden_states = encoder_output.last_hidden_state\n",
    "        hidden_size = hidden_states.shape[-1]\n",
    "\n",
    "        x = nn.Dropout(rate=self.dropout_rate, deterministic=not training)(hidden_states)\n",
    "        x_conv_input = jnp.transpose(x, (0, 2, 1))\n",
    "        x_conv = nn.Conv(features=hidden_size, kernel_size=(self.kernel_size,),\n",
    "                         padding='SAME', name='conv1d')(x_conv_input)\n",
    "        x_conv = nn.BatchNorm(use_running_average=not training, name='bn_conv')(x_conv)\n",
    "        x_conv = nn.relu(x_conv)\n",
    "        x_lstm_input = jnp.transpose(x_conv, (0, 2, 1))\n",
    "\n",
    "        lstm_out = x_lstm_input\n",
    "        for i in range(self.num_lstm_layers):\n",
    "            lstm_out, _ = nn.LSTM(features=hidden_size // 2,\n",
    "                                  bidirectional=True,\n",
    "                                  dropout_rate=self.lstm_dropout_rate,\n",
    "                                  name=f'lstm_layer_{i}',\n",
    "                                  )(lstm_out, None, deterministic=not training)\n",
    "\n",
    "        logits = nn.Dense(features=self.num_labels, name='classifier')(lstm_out)\n",
    "        logits = nn.Dropout(rate=self.dropout_rate, deterministic=not training)(logits)\n",
    "        crf_mask = attention_mask.astype(jnp.bool_)\n",
    "        crf_layer = CRF(num_tags=self.num_labels, name='crf')\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = crf_layer(logits, labels, mask=crf_mask)\n",
    "            predictions = crf_layer.viterbi_decode(logits, mask=crf_mask)\n",
    "            return loss, predictions\n",
    "        else:\n",
    "            predictions = crf_layer.viterbi_decode(logits, mask=crf_mask)\n",
    "            return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- State Management (using Optax and Flax's TrainState pattern) ---\n",
    "from flax.training import train_state\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    batch_stats: Any # To store BatchNorm moving averages/variances\n",
    "\n",
    "# Create a new TrainState with the model parameters, optimizer, and batch stats\n",
    "def create_model_and_params(key, encoder, num_labels, batch_size, seq_len):\n",
    "    dummy_input_ids = jnp.ones((batch_size, seq_len), dtype=jnp.int32)\n",
    "    dummy_attention_mask = jnp.ones((batch_size, seq_len), dtype=jnp.int32)\n",
    "\n",
    "    model = SPCNNClassifier(encoder=encoder, num_labels=num_labels)\n",
    "    variables = model.init(\n",
    "        {'params': key, 'dropout': jax.random.split(key)[0]},\n",
    "        input_ids=dummy_input_ids,\n",
    "        attention_mask=dummy_attention_mask,\n",
    "        labels=jnp.ones((batch_size, seq_len), dtype=jnp.int32),\n",
    "        training=True,\n",
    "        mutable=['batch_stats']\n",
    "    )\n",
    "    return model, variables['params'], variables['batch_stats']\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch, model):\n",
    "    \"\"\"\n",
    "    Performs a single training step.\n",
    "\n",
    "    Args:\n",
    "        state: Current TrainState containing model parameters, optimizer state, etc.\n",
    "        batch: Dictionary containing 'input_ids', 'attention_mask', and 'labels'.\n",
    "        model: The SPCNNClassifier instance.\n",
    "\n",
    "    Returns:\n",
    "        A new TrainState, the computed loss, and predictions.\n",
    "    \"\"\"\n",
    "    key, dropout_key = jax.random.split(state.rng) # Split RNG for next step and dropout\n",
    "\n",
    "    def loss_fn(params):\n",
    "        # `model.apply` expects a dictionary of variable collections.\n",
    "        # We pass 'params' for trainable weights and 'batch_stats' for BatchNorm state.\n",
    "        variables = {'params': params, 'batch_stats': state.batch_stats}\n",
    "        \n",
    "        # Call the model in training mode.\n",
    "        # `mutable=['batch_stats']` means `updated_variables['batch_stats']` will contain the new stats.\n",
    "        # `rngs={'dropout': dropout_key}` explicitly passes the key for dropout.\n",
    "        loss, predictions = model.apply(\n",
    "            variables,\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            labels=batch['labels'],\n",
    "            training=True,\n",
    "            mutable=['batch_stats'],\n",
    "            rngs={'dropout': dropout_key}\n",
    "        )\n",
    "        return loss, (predictions, variables['batch_stats']) # Return loss and auxiliary data\n",
    "\n",
    "    # Compute loss and gradients\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (predictions, updated_batch_stats)), grads = grad_fn(state.params)\n",
    "\n",
    "    # Apply updates to parameters using the optimizer\n",
    "    updates, new_opt_state = state.tx.update(grads, state.opt_state, state.params)\n",
    "    new_params = optax.apply_updates(state.params, updates)\n",
    "\n",
    "    # Update the training state\n",
    "    new_state = state.replace(\n",
    "        params=new_params,\n",
    "        opt_state=new_opt_state,\n",
    "        batch_stats=updated_batch_stats, # Update BatchNorm stats\n",
    "        rng=key # Update the PRNGKey for the next step\n",
    "    )\n",
    "    return new_state, loss, predictions\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state, batch, model):\n",
    "    \"\"\"\n",
    "    Performs a single evaluation step (no gradient updates).\n",
    "\n",
    "    Args:\n",
    "        state: Current TrainState (used for parameters and batch_stats).\n",
    "        batch: Dictionary containing 'input_ids', 'attention_mask', and 'labels'.\n",
    "        model: The SPCNNClassifier instance.\n",
    "\n",
    "    Returns:\n",
    "        The computed loss and predictions.\n",
    "    \"\"\"\n",
    "    # Use the current parameters and the running batch statistics for evaluation.\n",
    "    variables = {'params': state.params, 'batch_stats': state.batch_stats}\n",
    "    \n",
    "    # Call the model in evaluation mode (`training=False`).\n",
    "    # No mutable state updates, no dropout active.\n",
    "    loss, predictions = model.apply(\n",
    "        variables,\n",
    "        input_ids=batch['input_ids'],\n",
    "        attention_mask=batch['attention_mask'],\n",
    "        labels=batch['labels'],\n",
    "        training=False, # Important: disable training for evaluation (e.g., for BatchNorm, dropout)\n",
    "    )\n",
    "    return loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper func for gradual unfreezing\n",
    "def get_encoder_mask(params, encoder_key='encoder', unfreeze_layers=None):\n",
    "    def mask_fn(param_name, _):\n",
    "        if encoder_key in param_name:\n",
    "            if unfreeze_layers is None:\n",
    "                return False\n",
    "            return any(layer in param_name for layer in unfreeze_layers)\n",
    "        return True\n",
    "    return jax.tree_util.tree_map_with_path(mask_fn, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Training and Evaluation Loop\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    key = jax.random.PRNGKey(0) # Initial PRNG key\n",
    "    num_labels = 6\n",
    "    batch_size = 16\n",
    "    seq_len = MAX_LENGTH\n",
    "    learning_rate = 1e-4\n",
    "    num_epochs = 6\n",
    "\n",
    "    print(f\"Initializing model with encoder: {ENCODER}\")\n",
    "\n",
    "    # Initialize model and parameters\n",
    "    model, initial_params, initial_batch_stats = create_model_and_params(key, ENCODER, num_labels, batch_size, seq_len)\n",
    "\n",
    "    # using adamw optimizer and freezing the encoder initially\n",
    "    mask = get_encoder_mask(initial_params, encoder_key='encoder', unfreeze_layers=None)\n",
    "    tx = optax.adamw(learning_rate)\n",
    "    opt_state = tx.init(initial_params)\n",
    "\n",
    "    # Create initial training state\n",
    "    state = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=initial_params,\n",
    "        tx=tx,\n",
    "        opt_state=opt_state,\n",
    "        batch_stats=initial_batch_stats,\n",
    "        rng=key # Initial PRNGKey for training steps\n",
    "    )\n",
    "\n",
    "    print(\"Model and optimizer initialized successfully.\")\n",
    "\n",
    "    # Training loop using real data\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Unfreeze layers gradually\n",
    "        if epoch == 1:\n",
    "            unfreeze_layers = [('encoder', 'block', '0')]\n",
    "            mask = get_encoder_mask(state.params, encoder_key='encoder', unfreeze_layers=unfreeze_layers)\n",
    "            new_tx = optax.masked(optax.adamw(learning_rate), mask)\n",
    "            state = state.replace(tx=new_tx)\n",
    "            print(\"Unfroze encoder block 0.\")\n",
    "        elif epoch == 4:\n",
    "            unfreeze_layers = [('encoder', 'block', str(i)) for i in range(3)]\n",
    "            mask = get_encoder_mask(state.params, encoder_key='encoder', unfreeze_layers=unfreeze_layers)\n",
    "            new_tx = optax.masked(optax.adamw(learning_rate), mask)\n",
    "            state = state.replace(tx=new_tx)\n",
    "            print(\"Unfroze 0-2 encoder blocks.\")\n",
    "        elif epoch == 6:\n",
    "            unfreeze_layers = [('encoder', 'block', str(i)) for i in range(4)]\n",
    "            mask = get_encoder_mask(state.params, encoder_key='encoder', unfreeze_layers=unfreeze_layers)\n",
    "            new_tx = optax.masked(optax.adamw(learning_rate), mask)\n",
    "            state = state.replace(tx=new_tx)\n",
    "            print(\"Unfroze 0-3 encoder blocks.\")\n",
    "\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        train_losses = []\n",
    "        for batch in train_batches:\n",
    "            state, loss, predictions = train_step(state, batch, model)\n",
    "            train_losses.append(loss)\n",
    "        print(f\"  Train Loss: {np.mean(train_losses):.4f}\")\n",
    "\n",
    "        eval_losses = []\n",
    "        for batch in test_batches:\n",
    "            eval_loss, eval_predictions = eval_step(state, batch, model)\n",
    "            eval_losses.append(eval_loss)\n",
    "        print(f\"  Eval Loss: {np.mean(eval_losses):.4f}\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "\n",
    "    # test for using interference TODO maybe remove\n",
    "    print(\"\\nPerforming inference on a new sample:\")\n",
    "    inference_batch = test_batches[0]  # Use the first test batch as an example\n",
    "    inference_predictions = model.apply(\n",
    "        {'params': state.params, 'batch_stats': state.batch_stats},\n",
    "        input_ids=inference_batch['input_ids'],\n",
    "        attention_mask=inference_batch['attention_mask'],\n",
    "        training=False\n",
    "    )\n",
    "    print(f\"Inference prediction for a new sample (first sample): {inference_predictions[0]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/jonas-tfo/signal-peptide-prediction-models/blob/main/multiclass/sp_classifier_v7.ipynb",
     "timestamp": 1750670108382
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
