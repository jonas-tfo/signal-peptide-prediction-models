{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589df97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MODEL_NAME = \"Rostlab/prot_bert\" \n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\" # using mps instead of cuda for training on mac\n",
    "#DEVICE = \"cpu\"  # use GPU if available, otherwise CPU\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "NUM_CLASSES = 2  \n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67baaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "records = []  # uniprot_ac, kingdom, type_, sequence, label\n",
    "with open(\"/Users/jonas/Desktop/Uni/PBL/data/complete_set_unpartitioned.fasta\", \"r\") as f:\n",
    "    current_record = None\n",
    "    for line in f:\n",
    "        if line.startswith(\">\"):\n",
    "            if current_record is not None:\n",
    "                if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
    "                    # Save the previous record before starting a new one\n",
    "                    records.append(current_record)\n",
    "                else:\n",
    "                    # If the previous record is incomplete, skip it\n",
    "                    print(\"Skipping incomplete record:\", current_record)\n",
    "            # Start a new record\n",
    "            uniprot_ac, kingdom, type_ = line[1:].strip().split(\"|\")\n",
    "            current_record = {\"uniprot_ac\": uniprot_ac, \"kingdom\": kingdom, \"type\": type_, \"sequence\": None, \"label\": None}\n",
    "        else:\n",
    "            # Check if the line contains a sequence or a label\n",
    "            if current_record[\"sequence\"] is None:\n",
    "                current_record[\"sequence\"] = line.strip()\n",
    "            elif current_record[\"label\"] is None:\n",
    "                current_record[\"label\"] = line.strip()\n",
    "            else:\n",
    "                # If both sequence and label are already set, skip this line\n",
    "                print(\"Skipping extra line in record:\", current_record)\n",
    "    # Save the last record if it's complete\n",
    "    if current_record is not None:\n",
    "        if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
    "            records.append(current_record)\n",
    "        else:\n",
    "            print(\"Skipping incomplete record:\", current_record)\n",
    "\n",
    "\"\"\"\n",
    "# Save the DataFrame to a CSV file\n",
    "df_raw.to_csv(\"/Users/jonas/Desktop/Uni/PBL/data/complete_set_unpartitioned.csv\", index=False)\n",
    "\"\"\"\n",
    "# Print the number of records\n",
    "print(f\"Total records: {len(records)}\")\n",
    "df_raw = pd.DataFrame(records)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw[~df_raw[\"label\"].str.contains(\"P\")]\n",
    "df[\"type\"] = df[\"type\"].replace(\"NO_SP\", \"0\")\n",
    "df[\"type\"] = df[\"type\"].replace(\"LIPO\", \"1\")\n",
    "df[\"type\"] = df[\"type\"].replace(\"SP\", \"1\")\n",
    "df[\"type\"] = df[\"type\"].replace(\"TAT\", \"1\")\n",
    "df[\"type\"] = df[\"type\"].replace(\"TATLIPO\", \"1\")\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a439f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'S': 0, 'T': 1, 'L': 2, 'I': 3, 'M': 4, 'O': 5}\n",
    "\n",
    "df_encoded = df.copy()\n",
    "df_encoded[\"label\"] = df_encoded[\"label\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
    "df_encoded = df_encoded[df_encoded[\"label\"].map(len) > 0]  # Remove rows with empty label lists\n",
    "\n",
    "# make random smaller dataset\n",
    "#df_encoded = df_encoded.sample(frac=0.4, random_state=42)\n",
    "\n",
    "sequences = df_encoded[\"sequence\"].tolist()\n",
    "types = df_encoded[\"type\"].tolist()\n",
    "\n",
    "df_encoded.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd2c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "encoder = BertModel.from_pretrained(MODEL_NAME)\n",
    "encoder.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify by sequence length to avoid ValueError\n",
    "# TODO maybe change the stratify\n",
    "train_seqs, test_seqs, train_label_seqs, test_label_seqs = train_test_split(\n",
    "    sequences, types, test_size=0.3, random_state=42, stratify=types\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data \n",
    "class SPDataset(Dataset):\n",
    "    def __init__(self, sequences, label_seqs, label_map):\n",
    "        self.label_map = label_map\n",
    "        self.label_seqs = label_seqs\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        # preprocess the sequence (insert spaces between amino acids)\n",
    "        seq_processed = \" \".join(list(seq))\n",
    "        # Convert the single label string to an integer\n",
    "        label_value = int(self.label_seqs[idx])\n",
    "        # Tokenize the sequence (padding to ensure all sequences are the same length -> 512 tokens)\n",
    "        encoded = tokenizer(seq_processed, return_tensors=\"pt\",\n",
    "                            padding=\"max_length\", truncation=True, max_length=512)\n",
    "        input_ids = encoded['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Build a label tensor of the same length as input_ids.\n",
    "        # For tokens beyond the original sequence length, assign -100 so that loss func ignores them.\n",
    "        orig_length = len(seq)\n",
    "        token_labels = []\n",
    "        \n",
    "        for i in range(input_ids.size(0)):\n",
    "            if i == 0 or i > orig_length:\n",
    "                token_labels.append(-100)  # ignore padding tokens\n",
    "            else:\n",
    "                token_labels.append(label_value)\n",
    "        labels_tensor = torch.tensor(token_labels)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,  # tokenized and padded \n",
    "            'attention_mask': attention_mask,  # differentiate between padding and non-padding tokens\n",
    "            'labels': labels_tensor  # aligned label tensor\n",
    "        }\n",
    "\n",
    "train_dataset = SPDataset(train_seqs, train_label_seqs, label_map)\n",
    "test_dataset = SPDataset(test_seqs, test_label_seqs, label_map)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf5263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchcrf import CRF\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, encoder_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder_model  \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "        # 2 layer long short term memory network\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=512, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        # dense layer\n",
    "        self.classifier = nn.Linear(512 * 2, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Encode with BERT\n",
    "        encoder_output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = encoder_output.last_hidden_state  # (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Apply BiLSTM\n",
    "        lstm_out, _ = self.lstm(hidden_states)            # (batch, seq_len, 1024)\n",
    "\n",
    "        # Classifier to num_labels\n",
    "        x_linear = self.classifier(lstm_out)             # (batch, seq_len, num_labels)\n",
    "\n",
    "        x_linear = self.relu(x_linear)     # Apply ReLU activation\n",
    "\n",
    "        logits = self.dropout(x_linear)     # (batch, seq_len, num_labels)\n",
    "\n",
    "        if labels is not None:\n",
    "            # Replace ignore-index (-100) with a valid label (0) since CRF doesn't support -100\n",
    "            mod_labels = labels.clone()\n",
    "            mod_labels[labels == -100] = 0\n",
    "            loss = -self.crf(logits, mod_labels, mask=attention_mask.bool(), reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            predictions = self.crf.decode(logits, mask=attention_mask.bool())\n",
    "            return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0e42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Initialize the model\n",
    "model = BinaryClassifier(encoder, NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# optimizer \n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model.encoder.encoder.layer[-4:].parameters(), \"lr\": 5e-6},\n",
    "    {\"params\": model.classifier.parameters(), \"lr\": 1e-3},\n",
    "    {\"params\": model.lstm.parameters(), \"lr\": 1e-3},\n",
    "    {\"params\": model.crf.parameters(), \"lr\": 1e-3},\n",
    "])  # adjust weight_decay as needed\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "# scheduler for learning rate\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdec412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "import gc\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"/Users/jonas/Desktop/Uni/PBL/logs/bert_sp_binary_classifier\")\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Freeze encoder parameters initially (last 10 layers)\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\")\n",
    "    total_loss = 0  # total epoch loss\n",
    "\n",
    "    for batch in pbar:\n",
    "        try:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            token_labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()  # reset gradients\n",
    "\n",
    "            loss = model(input_ids, attention_mask, token_labels)  # single forward pass\n",
    "\n",
    "            scaler.scale(loss).backward()      # backpropagation\n",
    "            scaler.unscale_(optimizer)         # Unscale gradients before clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # gradient clipping\n",
    "            scaler.step(optimizer)             # update weights\n",
    "            scaler.update()                    # update scaler\n",
    "            scheduler.step()                   # update learning rate\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(\"Error during training:\", e)\n",
    "            gc.collect()\n",
    "            torch.mps.empty_cache()\n",
    "            continue\n",
    "\n",
    "    torch.mps.empty_cache()  \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_train_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score, matthews_corrcoef, accuracy_score, classification_report\n",
    "\n",
    "model = BinaryClassifier(encoder, NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "val_loss = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        # Compute loss using CRF (pass labels)\n",
    "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # Decode predictions using CRF (no labels passed)\n",
    "        predictions = model(input_ids=input_ids, attention_mask=attention_mask)  # List[List[int]]\n",
    "\n",
    "        # Loop through batch and collect valid tokens\n",
    "        for pred_seq, label_seq, mask in zip(predictions, labels, attention_mask):\n",
    "            for pred, true, is_valid in zip(pred_seq, label_seq, mask):\n",
    "                if true.item() != -100 and is_valid.item() == 1:\n",
    "                    all_preds.append(pred)\n",
    "                    all_labels.append(true.item())\n",
    "\n",
    "\n",
    "# Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=list(label_map.keys())))\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=list(label_map.keys()), yticklabels=list(label_map.keys()))\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.savefig(\"/Users/jonas/Desktop/Uni/PBL/logs/confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "# F1 Score weighted\n",
    "from sklearn.metrics import classification_report, f1_score, matthews_corrcoef, accuracy_score, classification_report\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(f\"F1 Score (weighted): {f1:.4f}\")\n",
    "\n",
    "# F1 Score macro\n",
    "f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC)\n",
    "mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
    "\n",
    "\n",
    "writer.add_scalar(\"Loss/test\", val_loss)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
