{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04169412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MODEL_NAME = \"Rostlab/prot_bert\" \n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\" # using mps instead of cuda for training on mac\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "NUM_CLASSES = 6  # num classes for classification\n",
    "BATCH_SIZE = 44\n",
    "EPOCHS = 10\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc725e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "records = []  # uniprot_ac, kingdom, type_, sequence, label\n",
    "with open(\"/Users/jonas/Desktop/Uni/PBL/data/complete_set_unpartitioned.fasta\", \"r\") as f:\n",
    "    current_record = None\n",
    "    for line in f:\n",
    "        if line.startswith(\">\"):\n",
    "            if current_record is not None:\n",
    "                if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
    "                    # Save the previous record before starting a new one\n",
    "                    records.append(current_record)\n",
    "                else:\n",
    "                    # If the previous record is incomplete, skip it\n",
    "                    print(\"Skipping incomplete record:\", current_record)\n",
    "            # Start a new record\n",
    "            uniprot_ac, kingdom, type_ = line[1:].strip().split(\"|\")\n",
    "            current_record = {\"uniprot_ac\": uniprot_ac, \"kingdom\": kingdom, \"type\": type_, \"sequence\": None, \"label\": None}\n",
    "        else:\n",
    "            # Check if the line contains a sequence or a label\n",
    "            if current_record[\"sequence\"] is None:\n",
    "                current_record[\"sequence\"] = line.strip()\n",
    "            elif current_record[\"label\"] is None:\n",
    "                current_record[\"label\"] = line.strip()\n",
    "            else:\n",
    "                # If both sequence and label are already set, skip this line\n",
    "                print(\"Skipping extra line in record:\", current_record)\n",
    "    # Save the last record if it's complete\n",
    "    if current_record is not None:\n",
    "        if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
    "            records.append(current_record)\n",
    "        else:\n",
    "            print(\"Skipping incomplete record:\", current_record)\n",
    "\n",
    "\"\"\"\n",
    "# Save the DataFrame to a CSV file\n",
    "df_raw.to_csv(\"/Users/jonas/Desktop/Uni/PBL/data/complete_set_unpartitioned.csv\", index=False)\n",
    "\"\"\"\n",
    "# Print the number of records\n",
    "print(f\"Total records: {len(records)}\")\n",
    "df_raw = pd.DataFrame(records)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw[~df_raw[\"label\"].str.contains(\"P\")]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568deead",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'S': 0, 'T': 1, 'L': 2, 'I': 3, 'M': 4, 'O': 5}\n",
    "\n",
    "df_encoded = df.copy()\n",
    "df_encoded[\"label\"] = df_encoded[\"label\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
    "df_encoded = df_encoded[df_encoded[\"label\"].map(len) > 0]  # Remove rows with empty label lists\n",
    "\n",
    "# make random smaller dataset\n",
    "#df_encoded = df_encoded.sample(frac=0.4, random_state=42)\n",
    "\n",
    "sequences = df_encoded[\"sequence\"].tolist()\n",
    "label_seqs = df_encoded[\"label\"].tolist()\n",
    "\n",
    "df_encoded.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bfa5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "encoder = BertModel.from_pretrained(MODEL_NAME)\n",
    "encoder.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcdebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify by sequence length to avoid ValueError\n",
    "train_seqs, test_seqs, train_label_seqs, test_label_seqs = train_test_split(\n",
    "    sequences, label_seqs, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data \n",
    "class SPDataset(Dataset):\n",
    "    def __init__(self, sequences, label_seqs, label_map):\n",
    "        self.label_map = label_map\n",
    "        self.label_seqs = label_seqs\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        # preprocess the sequence (insert spaces between amino acids)\n",
    "        seq_processed = \" \".join(list(seq))\n",
    "        labels = self.label_seqs[idx]\n",
    "        # Tokenize the sequence (padding to ensure all sequences are the same length -> 512 tokens) \n",
    "        encoded = tokenizer(seq_processed, return_tensors=\"pt\",\n",
    "                            padding=\"max_length\", truncation=True, max_length=512)\n",
    "        input_ids = encoded['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Build a label tensor of the same length as input_ids.\n",
    "        # For tokens beyond the original sequence length, assign -100 so that loss func ignores them.\n",
    "        orig_length = len(seq)\n",
    "        token_labels = []\n",
    "        \n",
    "        for i in range(input_ids.size(0)):\n",
    "            if i == 0 or i > orig_length:  \n",
    "                token_labels.append(-100)  # ignore padding tokens\n",
    "            else:\n",
    "                # Use the already encoded label directly\n",
    "                token_labels.append(labels[i-1])\n",
    "        labels_tensor = torch.tensor(token_labels)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids, # tokenized and padded \n",
    "            'attention_mask': attention_mask, # differentiate between padding and non-padding tokens\n",
    "            'labels': labels_tensor # aligned label tensor\n",
    "        }\n",
    "\n",
    "train_dataset = SPDataset(train_seqs, train_label_seqs, label_map)\n",
    "test_dataset = SPDataset(test_seqs, test_label_seqs, label_map)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd916c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the model on top of the prot_bert encoder\n",
    "# using the encoder as a protein feature extractor\n",
    "# and adding a one dimensional convolutional layer on top of it \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SPCNNClassifier(nn.Module):\n",
    "    def __init__(self, encoder_model):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder_model  # encoder: ProtBERT model\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        # First convolution: map features to 1024 channels (match the hidden size)\n",
    "        self.conv1 = nn.Conv1d(in_channels=hidden_size, out_channels=1024, kernel_size=6, dilation=2, padding=5)\n",
    "        # Second convolution: map features to the 6 classes for each token\n",
    "        self.conv2 = nn.Conv1d(in_channels=1024, out_channels=NUM_CLASSES, kernel_size=3, dilation=2, padding=2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        encoder_output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = encoder_output.last_hidden_state  # shape: (batch, seq_len, hidden)\n",
    "        \n",
    "        x = hidden_states.transpose(1, 2)         # Transpose to (batch, hidden, seq_len) for 1D convolution\n",
    "        x = self.conv1(x)         # shape: (batch, 256, seq_len)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)         # shape: (batch, NUM_CLASSES, seq_len)\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(1, 2)         # Transpose back to (batch, seq_len, NUM_CLASSES)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Initialize the model\n",
    "model = SPCNNClassifier(encoder).to(DEVICE)\n",
    "\n",
    "# optimizer and Loss\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LR,           # Learning rate\n",
    "    betas=(0.85, 0.999),  # momentum, can overshoot \n",
    "    eps=1e-6,          # epsilon\n",
    "    weight_decay=0.01  # regularization to prevent overfitting\n",
    ")\n",
    "\n",
    "# scheduler for learning rate\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(train_loader) * EPOCHS\n",
    ")\n",
    "\n",
    "# Counter({'I': 1204001, 'O': 362643, 'S': 85526, 'M': 74445, 'L': 46065, 'T': 22272, 'P': 951})\n",
    "class_counts = [1204001, 85526, 22272, 46865, 74445, 362643]  # Count for each class (I, S, T, L, M, O)\n",
    "# hopefully deals with the class imbalance\n",
    "weights = torch.tensor([1.0 / count for count in class_counts], device=DEVICE)\n",
    "\n",
    "# loss function that ignores the padding tokens (-100)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights, ignore_index=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader)):\n",
    "            # if i == 10:\n",
    "            #     break\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            predictions = model(input_ids=input_ids, attention_mask=attention_mask)  # batch, seq_len\n",
    "            for pred, label in zip(predictions, labels):\n",
    "                if len(pred) != len(label):\n",
    "                    label = label[:len(pred)]\n",
    "                all_predictions.append(pred)\n",
    "                all_labels.append(label)\n",
    "\n",
    "    flattened_predictions = [item for sublist in all_predictions for item in sublist]\n",
    "    flattened_labels = [label_map[item] for sublist in all_labels for item in sublist]\n",
    "\n",
    "    val_loss = total_loss / len(dataloader)\n",
    "    token_acc = accuracy_score(flattened_labels, flattened_predictions)\n",
    "\n",
    "    seq_acc = sum(\n",
    "        [pred == label for pred, label in zip(all_predictions, all_labels)]\n",
    "    ) / len(labels)\n",
    "\n",
    "    return val_loss, token_acc, seq_acc, classification_report(flattened_labels, flattened_predictions, output_dict=True, zero_division=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"/Users/jonas/Desktop/Uni/PBL/logs/prot_bert_linear_classifier_v2\")\n",
    "\n",
    "# Training Loop\n",
    "model.train()\n",
    "\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Gradually unfreeze only a subset of encoder layers for efficiency\n",
    "    if epoch == 4:\n",
    "        # Unfreeze only the last encoder layer\n",
    "        for param in model.encoder.encoder.layer[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "    elif epoch == 7:\n",
    "        # Optionally unfreeze one more layer rather than the full encoder\n",
    "        for param in model.encoder.encoder.layer[-2].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    total_loss = 0 # total epoch loss\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        token_labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()  # resets gradients\n",
    "        logits = model(input_ids, attention_mask)  # forward pass\n",
    "        loss = loss_fn(logits.reshape(-1, NUM_CLASSES), token_labels.reshape(-1)) # flatten logits and labels and compute loss\n",
    "        loss.backward()  # backpropagation\n",
    "        optimizer.step()  # update weights with optimizer\n",
    "        scheduler.step() # update learning rate\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_train_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "\n",
    "    # Run evaluation at the end of the epoch\n",
    "    model.eval()\n",
    "    val_loss, token_acc, seq_acc, report = evaluate(model, test_loader, DEVICE)\n",
    "    print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Token Acc: {token_acc:.4f}, Seq Acc: {seq_acc:.4f}\")\n",
    "    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", token_acc, epoch)\n",
    "    writer.add_scalar(\"Seq_Accuracy/val\", seq_acc, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "all_preds = [] # predicted labels\n",
    "all_labels = [] # true types\n",
    "with torch.no_grad():\n",
    "    # uses the batches from the test set for eval\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids, attention_mask) # runs forward pass\n",
    "        preds = torch.argmax(logits, dim=-1) # get per token predicted labels\n",
    "\n",
    "        loss = loss_fn(logits.view(-1, NUM_CLASSES), labels.view(-1)) # calculate loss\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # flatten the predictions and labels\n",
    "        preds_flat = preds.view(-1)\n",
    "        labels_flat = labels.view(-1)\n",
    "        valid_idx = labels_flat != -100 # exclude padding tokens (-100)\n",
    "        all_preds.extend(preds_flat[valid_idx].cpu().numpy())\n",
    "        all_labels.extend(labels_flat[valid_idx].cpu().numpy())\n",
    "\n",
    "# Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=list(label_map.keys())))\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# MCC\n",
    "mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=list(label_map.values()))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_map.keys())\n",
    "disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# For TensorBoard\n",
    "avg_val_loss = val_loss / len(test_loader)\n",
    "val_acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Epoch {epoch+1}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "writer.add_scalar(\"Loss/val\", avg_val_loss, epoch)\n",
    "writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ee4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), \"/Users/jonas/Desktop/Uni/PBL/models/prot_bert_linear_classifier_v2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e612fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sequence-level accuracy, skipping -100 (ignored) positions\n",
    "def sequence_level_accuracy(preds_flat, labels_flat, test_label_seqs):\n",
    "    # Step 1: Reconstruct sequence-wise predictions and labels\n",
    "    seq_lengths = [len(seq) for seq in test_label_seqs]\n",
    "    preds_seq = []\n",
    "    labels_seq = []\n",
    "    idx = 0\n",
    "    for l in seq_lengths:\n",
    "        preds_seq.append(preds_flat[idx:idx+l])\n",
    "        labels_seq.append(labels_flat[idx:idx+l])\n",
    "        idx += l\n",
    "\n",
    "    # Step 2: Compute sequence-level accuracy using is_valid variable\n",
    "    correct = 0\n",
    "    for pred, label in zip(preds_seq, labels_seq):\n",
    "        is_valid = [l != -100 for l in label]\n",
    "        valid_preds = [p for p, valid in zip(pred, is_valid) if valid]\n",
    "        valid_labels = [l for l, valid in zip(label, is_valid) if valid]\n",
    "        if valid_preds == valid_labels:\n",
    "            correct += 1\n",
    "\n",
    "    total = len(seq_lengths)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "acc = sequence_level_accuracy(all_preds, all_labels, test_label_seqs)\n",
    "print(f\"Sequence Level Accuracy: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
